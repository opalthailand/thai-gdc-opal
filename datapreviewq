[1mdiff --git a/ckanext/thai_gdc/actions/exporter_action.py b/ckanext/thai_gdc/actions/exporter_action.py[m
[1mindex 9c75312..d0868ff 100644[m
[1m--- a/ckanext/thai_gdc/actions/exporter_action.py[m
[1m+++ b/ckanext/thai_gdc/actions/exporter_action.py[m
[36m@@ -49,10 +49,10 @@[m [mcolumns = [[m
         'field': 'data_category',[m
     },[m
     {[m
[31m-        'field': 'data_class_level',[m
[32m+[m[32m        'field': 'license_id',[m
     },[m
     {[m
[31m-        'field': 'license_id',[m
[32m+[m[32m        'field': 'data_classification',[m
     },[m
     {[m
         'field': 'accessible_condition',[m
[1mdiff --git a/ckanext/thai_gdc/actions/opend_action.py b/ckanext/thai_gdc/actions/opend_action.py[m
[1mindex b246433..e2292ee 100644[m
[1m--- a/ckanext/thai_gdc/actions/opend_action.py[m
[1m+++ b/ckanext/thai_gdc/actions/opend_action.py[m
[36m@@ -7,7 +7,6 @@[m [mimport ckan.logic.schema as schema_[m
 import ckan.model as model[m
 import logging[m
 import ckan.plugins.toolkit as toolkit[m
[31m-from ckanext.thai_gdc.controllers.dataset import DatasetImportController[m
 from ckan.lib.jobs import DEFAULT_QUEUE_NAME[m
 import ckan.lib.dictization as d[m
 import ckan.lib.dictization.model_dictize as model_dictize[m
[36m@@ -18,8 +17,8 @@[m [mimport ckan.model.misc as misc[m
 from ckan.common import config[m
 import ckan[m
 from ckanext.thai_gdc import helpers as thai_gdc_h[m
[32m+[m[32mfrom ckanext.thai_gdc import blueprint as thai_gdc_b[m[41m[m
 from sqlalchemy import func[m
[31m-import ckan.lib.datapreview as datapreview[m
 [m
 _check_access = logic.check_access[m
 _get_or_bust = logic.get_or_bust[m
[36m@@ -152,19 +151,18 @@[m [mdef dataset_bulk_import(context, data_dict):[m
     _check_access('package_create', context, data_dict)[m
     import_uuid = _get_or_bust(data_dict, 'import_uuid')[m
     queue = DEFAULT_QUEUE_NAME[m
[31m-    dataset_import = DatasetImportController()[m
     [m
[31m-    toolkit.enqueue_job(dataset_import._record_type_process, [data_dict], title=u'import record package import_id:{}'.format(import_uuid), queue=queue)[m
[32m+[m[32m    toolkit.enqueue_job(thai_gdc_b._record_type_process, [data_dict], title=u'import record package import_id:{}'.format(import_uuid), queue=queue)[m[41m[m
                 [m
[31m-    toolkit.enqueue_job(dataset_import._stat_type_process, [data_dict], title=u'import stat package import_id:{}'.format(import_uuid), queue=queue)[m
[32m+[m[32m    toolkit.enqueue_job(thai_gdc_b._stat_type_process, [data_dict], title=u'import stat package import_id:{}'.format(import_uuid), queue=queue)[m[41m[m
 [m
[31m-    toolkit.enqueue_job(dataset_import._gis_type_process, [data_dict], title=u'import gis package import_id:{}'.format(import_uuid), queue=queue)[m
[32m+[m[32m    toolkit.enqueue_job(thai_gdc_b._gis_type_process, [data_dict], title=u'import gis package import_id:{}'.format(import_uuid), queue=queue)[m[41m[m
 [m
[31m-    toolkit.enqueue_job(dataset_import._multi_type_process, [data_dict], title=u'import multi package import_id:{}'.format(import_uuid), queue=queue)[m
[32m+[m[32m    toolkit.enqueue_job(thai_gdc_b._multi_type_process, [data_dict], title=u'import multi package import_id:{}'.format(import_uuid), queue=queue)[m[41m[m
 [m
[31m-    toolkit.enqueue_job(dataset_import._other_type_process, [data_dict], title=u'import other package import_id:{}'.format(import_uuid), queue=queue)[m
[32m+[m[32m    toolkit.enqueue_job(thai_gdc_b._other_type_process, [data_dict], title=u'import other package import_id:{}'.format(import_uuid), queue=queue)[m[41m[m
 [m
[31m-    toolkit.enqueue_job(dataset_import._finished_process, [data_dict], title=u'import finished import_id:{}'.format(import_uuid), queue=queue)[m
[32m+[m[32m    toolkit.enqueue_job(thai_gdc_b._finished_process, [data_dict], title=u'import finished import_id:{}'.format(import_uuid), queue=queue)[m[41m[m
 [m
 def resource_view_create(context, data_dict):[m
     '''Creates a new resource view.[m
[1mdiff --git a/ckanext/thai_gdc/auth.py b/ckanext/thai_gdc/auth.py[m
[1mindex 7419284..145b6ab 100644[m
[1m--- a/ckanext/thai_gdc/auth.py[m
[1m+++ b/ckanext/thai_gdc/auth.py[m
[36m@@ -2,7 +2,7 @@[m
 # encoding: utf-8[m
 [m
 import ckan.plugins.toolkit as toolkit[m
[31m-from ckan.common import _, c[m
[32m+[m[32mfrom ckan.common import _[m[41m[m
 import ckan.logic.auth as logic_auth[m
 from ckan import logic[m
 import ckan.authz as authz[m
[36m@@ -53,8 +53,10 @@[m [mdef member_create(context, data_dict):[m
     # However if the user is member of group then they can add/remove datasets[m
     if not group.is_organization and data_dict.get('object_type') == 'package':[m
         permission = 'manage_group'[m
[32m+[m[41m    [m
[32m+[m[32m    blueprint, endpoint = toolkit.get_endpoint()[m[41m[m
 [m
[31m-    if c.controller in ['package', 'dataset'] and c.action in ['groups']:[m
[32m+[m[32m    if blueprint in ['package', 'dataset'] and endpoint in ['groups']:[m[41m[m
         authorized = thai_gdc_h.user_has_admin_access(include_editor_access=True)[m
         # Fallback to the default CKAN behaviour[m
         if not authorized:[m
[36m@@ -86,7 +88,8 @@[m [mdef package_delete(context, data_dict):[m
     # are essentially changing the state field[m
     try:[m
         gd_pkg_state = thai_gdc_h.get_gdcatalog_state('published', data_dict.get('id')).get('result')[m
[31m-        if gd_pkg_state[0].get('metadata_modified') != '' and toolkit.c.controller == 'dataset':[m
[32m+[m[32m        blueprint, endpoint = toolkit.get_endpoint()[m[41m[m
[32m+[m[32m        if gd_pkg_state[0].get('metadata_modified') != '' and blueprint in ['package', 'dataset']:[m[41m[m
             return {'success': False}[m
         else:[m
             return authz.is_authorized('package_update', context, data_dict)[m
[1mdiff --git a/ckanext/thai_gdc/ckan_dataset.json b/ckanext/thai_gdc/ckan_dataset.json[m
[1mindex 7784d29..7d21691 100644[m
[1m--- a/ckanext/thai_gdc/ckan_dataset.json[m
[1m+++ b/ckanext/thai_gdc/ckan_dataset.json[m
[36m@@ -306,7 +306,7 @@[m
           "en": "Update Frequency Interval",[m
           "th": "‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)"[m
         },[m
[31m-        "form_placeholder": "‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà (‡∏Ñ‡∏£‡∏±‡πâ‡∏á/‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà)",[m
[32m+[m[32m        "form_placeholder": "‡πÄ‡∏•‡∏Ç‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà",[m
         "form_data_type": ["‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô","‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥","‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏π‡∏°‡∏¥‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà","‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó","‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏≠‡∏∑‡πà‡∏ô‡πÜ"][m
       },[m
       {[m
[36m@@ -535,10 +535,10 @@[m
         "required": true[m
       },[m
       {[m
[31m-        "field_name": "data_class_level",[m
[32m+[m[32m        "field_name": "data_classification",[m
         "label": {[m
[31m-          "en": "Data Class Level",[m
[31m-          "th": "‡∏ä‡∏±‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê"[m
[32m+[m[32m          "en": "Data Classification",[m
[32m+[m[32m          "th": "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"[m
         },[m
         "choices": [[m
           {[m
[36m@@ -546,8 +546,8 @@[m
             "label": "‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ú‡∏¢"[m
           },[m
           {[m
[31m-            "value": "‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£",[m
[31m-            "label": "‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£"[m
[32m+[m[32m            "value": "‡∏ä‡∏±‡πâ‡∏ô‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£",[m
[32m+[m[32m            "label": "‡∏ä‡∏±‡πâ‡∏ô‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£"[m
           },[m
           {[m
             "value": "‡∏•‡∏±‡∏ö",[m
[36m@@ -562,7 +562,7 @@[m
             "label": "‡∏•‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"[m
           }[m
         ],[m
[31m-        "form_snippet": "data_class_level.html",[m
[32m+[m[32m        "form_snippet": "data_classification.html",[m
         "display_snippet": "select.html",[m
         "form_data_type": ["‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô","‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥","‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏π‡∏°‡∏¥‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà","‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó","‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏≠‡∏∑‡πà‡∏ô‡πÜ"][m
       },[m
[36m@@ -590,14 +590,14 @@[m
             "value": "Creative Commons Attribution Non-Commercial Share-Alike",[m
             "label": "Creative Commons Attribution Non-Commercial Share-Alike"[m
           },[m
[31m-          {[m
[31m-            "value": "Creative Commons Attribution No-Derivs",[m
[31m-            "label": "Creative Commons Attribution No-Derivs"[m
[31m-          },[m
           {[m
             "value": "Creative Commons Attribution Non-Commercial No-Derivs",[m
             "label": "Creative Commons Attribution Non-Commercial No-Derivs"[m
           },[m
[32m+[m[32m          {[m
[32m+[m[32m            "value": "Creative Commons Attribution No-Derivs",[m
[32m+[m[32m            "label": "Creative Commons Attribution No-Derivs"[m
[32m+[m[32m          },[m
           {[m
             "value": "‡∏≠‡∏∑‡πà‡∏ô‡πÜ",[m
             "label": "‡∏≠‡∏∑‡πà‡∏ô‡πÜ"[m
[1mdiff --git a/ckanext/thai_gdc/controllers/__init__.py b/ckanext/thai_gdc/controllers/__init__.py[m
[1mdeleted file mode 100644[m
[1mindex e69de29..0000000[m
[1mdiff --git a/ckanext/thai_gdc/controllers/banner.py b/ckanext/thai_gdc/controllers/banner.py[m
[1mdeleted file mode 100644[m
[1mindex 479a60e..0000000[m
[1m--- a/ckanext/thai_gdc/controllers/banner.py[m
[1m+++ /dev/null[m
[36m@@ -1,133 +0,0 @@[m
[31m-# -*- coding: utf-8 -*-[m
[31m-import ckan.plugins as p[m
[31m-import ckan.lib.helpers as helpers[m
[31m-from pylons import config[m
[31m-import ckan.logic as logic[m
[31m-import ckan.lib.navl.dictization_functions as dict_fns[m
[31m-import ckan.model as model[m
[31m-import ckan.lib.uploader as uploader[m
[31m-import six[m
[31m-import time[m
[31m-import logging[m
[31m-[m
[31m-from ckan.plugins.toolkit import ([m
[31m-    _, c, h, BaseController, check_access, NotAuthorized, abort, render,[m
[31m-    redirect_to, request,[m
[31m-    )[m
[31m-[m
[31m-from ckan.controllers.home import CACHE_PARAMETERS[m
[31m-import ckan.logic.schema as schema_[m
[31m-import ckan.lib.app_globals as app_globals[m
[31m-[m
[31m-_validate = dict_fns.validate[m
[31m-ValidationError = logic.ValidationError[m
[31m-[m
[31m-log = logging.getLogger(__name__)[m
[31m-[m
[31m-class BannerEditController(BaseController):[m
[31m-    [m
[31m-    def edit_banner(self):[m
[31m-[m
[31m-        context = {'model': model,[m
[31m-                   'user': c.user, 'auth_user_obj': c.userobj}[m
[31m-        try:[m
[31m-            check_access('config_option_update', context, {})[m
[31m-        except logic.NotAuthorized:[m
[31m-            abort(403, _('Need to be system administrator to administer'))[m
[31m-[m
[31m-        items = [[m
[31m-            {'name': 'ckan.promoted_banner', 'control': 'image_upload', 'label': _('Promoted banner'), 'placeholder': '', 'upload_enabled':h.uploads_enabled(),[m
[31m-                'field_url': 'ckan.promoted_banner', 'field_upload': 'promoted_banner_upload', 'field_clear': 'clear_promoted_banner_upload'},[m
[31m-            {'name': 'ckan.search_background', 'control': 'image_upload', 'label': _('Search background'), 'placeholder': '', 'upload_enabled':h.uploads_enabled(),[m
[31m-                'field_url': 'ckan.search_background', 'field_upload': 'search_background_upload', 'field_clear': 'clear_search_background_upload'},[m
[31m-            {'name': 'ckan.favicon', 'control': 'favicon_upload', 'label': _('Site favicon'), 'placeholder': '', 'upload_enabled':h.uploads_enabled(),[m
[31m-                'field_url': 'ckan.favicon', 'field_upload': 'favicon_upload', 'field_clear': 'clear_favicon_upload'},[m
[31m-        ][m
[31m-        data = request.POST[m
[31m-        if 'save' in data:[m
[31m-            try:[m
[31m-                # really?[m
[31m-                data_dict = logic.clean_dict([m
[31m-                    dict_fns.unflatten([m
[31m-                        logic.tuplize_dict([m
[31m-                            logic.parse_params([m
[31m-                                request.POST, ignore_keys=CACHE_PARAMETERS))))[m
[31m-[m
[31m-                del data_dict['save'][m
[31m-[m
[31m-                c.revision_change_state_allowed = True[m
[31m-[m
[31m-                schema = schema_.update_configuration_schema()[m
[31m-[m
[31m-                upload = uploader.get_uploader('admin')[m
[31m-                upload.update_data_dict(data_dict, 'ckan.promoted_banner',[m
[31m-                                    'promoted_banner_upload', 'clear_promoted_banner_upload')[m
[31m-                upload.upload(uploader.get_max_image_size())[m
[31m-            [m
[31m-                upload = uploader.get_uploader('admin')[m
[31m-                upload.update_data_dict(data_dict, 'ckan.search_background',[m
[31m-                                    'search_background_upload', 'clear_search_background_upload')[m
[31m-                upload.upload(uploader.get_max_image_size())[m
[31m-[m
[31m-                upload = uploader.get_uploader('admin')[m
[31m-                upload.update_data_dict(data_dict, 'ckan.favicon',[m
[31m-                                    'favicon_upload', 'clear_favicon_upload')[m
[31m-                upload.upload(uploader.get_max_image_size())[m
[31m-[m
[31m-                data, errors = _validate(data_dict, schema, context)[m
[31m-                if errors:[m
[31m-                    model.Session.rollback()[m
[31m-                    raise ValidationError(errors)[m
[31m-[m
[31m-                for key, value in six.iteritems(data):[m
[31m-                [m
[31m-                    if key == 'ckan.promoted_banner' and value and not value.startswith('http')\[m
[31m-                            and not value.startswith('/'):[m
[31m-                        image_path = 'uploads/admin/'[m
[31m-[m
[31m-                        value = h.url_for_static('{0}{1}'.format(image_path, value))[m
[31m-                    [m
[31m-                    if key == 'ckan.search_background' and value and not value.startswith('http')\[m
[31m-                            and not value.startswith('/'):[m
[31m-                        image_path = 'uploads/admin/'[m
[31m-[m
[31m-                        value = h.url_for_static('{0}{1}'.format(image_path, value))[m
[31m-                    [m
[31m-                    if key == 'ckan.favicon' and value and not value.startswith('http')\[m
[31m-                            and not value.startswith('/'):[m
[31m-                        image_path = 'uploads/admin/'[m
[31m-[m
[31m-                        value = h.url_for_static('{0}{1}'.format(image_path, value))[m
[31m-[m
[31m-                    # Save value in database[m
[31m-                    model.set_system_info(key, value)[m
[31m-[m
[31m-                    # Update CKAN's `config` object[m
[31m-                    config[key] = value[m
[31m-[m
[31m-                    # Only add it to the app_globals (`g`) object if explicitly defined[m
[31m-                    # there[m
[31m-                    globals_keys = app_globals.app_globals_from_config_details.keys()[m
[31m-                    if key in globals_keys:[m
[31m-                        app_globals.set_app_global(key, value)[m
[31m-[m
[31m-                # Update the config update timestamp[m
[31m-                model.set_system_info('ckan.config_update', str(time.time()))[m
[31m-[m
[31m-                log.info('Updated config options: {0}'.format(data))[m
[31m-            except logic.ValidationError as e:[m
[31m-                errors = e.error_dict[m
[31m-                error_summary = e.error_summary[m
[31m-                vars = {'data': data, 'errors': errors,[m
[31m-                        'error_summary': error_summary, 'form_items': items}[m
[31m-                return render('admin/banner_form.html', extra_vars=vars)[m
[31m-[m
[31m-            h.redirect_to(controller='ckanext.thai_gdc.controllers.banner:BannerEditController', action='edit_banner')[m
[31m-[m
[31m-        schema = logic.schema.update_configuration_schema()[m
[31m-        data = {}[m
[31m-        for key in schema:[m
[31m-            data[key] = config.get(key)[m
[31m-[m
[31m-        vars = {'data': data, 'errors': {}, 'form_items': items}[m
[31m-        return render('admin/banner_form.html', extra_vars=vars)[m
[1mdiff --git a/ckanext/thai_gdc/controllers/dataset.py b/ckanext/thai_gdc/controllers/dataset.py[m
[1mdeleted file mode 100644[m
[1mindex 95a73ce..0000000[m
[1m--- a/ckanext/thai_gdc/controllers/dataset.py[m
[1m+++ /dev/null[m
[36m@@ -1,946 +0,0 @@[m
[31m-# -*- coding: utf-8 -*-[m
[31m-import ckan.plugins as p[m
[31m-from pylons import config[m
[31m-import ckan.logic as logic[m
[31m-import ckan.lib.navl.dictization_functions as dict_fns[m
[31m-import ckan.model as model[m
[31m-import ckan.lib.uploader as uploader[m
[31m-import six[m
[31m-import logging[m
[31m-import pandas as pd[m
[31m-import numpy as np[m
[31m-from ckanapi import LocalCKAN[m
[31m-import datetime[m
[31m-[m
[31m-import uuid[m
[31m-import ckan.plugins.toolkit as toolkit[m
[31m-[m
[31m-from ckan.plugins.toolkit import ([m
[31m-    _, c, h, check_access, abort, render, request[m
[31m-    )[m
[31m-[m
[31m-from ckan.controllers.home import CACHE_PARAMETERS[m
[31m-import ckan.logic.schema as schema_[m
[31m-[m
[31m-import sys[m
[31m-reload(sys)[m
[31m-sys.setdefaultencoding('utf-8')[m
[31m-[m
[31m-_validate = dict_fns.validate[m
[31m-ValidationError = logic.ValidationError[m
[31m-NotFound = logic.NotFound[m
[31m-NotAuthorized = logic.NotAuthorized[m
[31m-[m
[31m-log = logging.getLogger(__name__)[m
[31m-[m
[31m-class DatasetManageController(p.toolkit.BaseController):[m
[31m-[m
[31m-    def datatype_patch(self, package_id):[m
[31m-        data = request.GET[m
[31m-        if 'data_type' in data:[m
[31m-            try:[m
[31m-                data_dict = logic.clean_dict([m
[31m-                    dict_fns.unflatten([m
[31m-                        logic.tuplize_dict([m
[31m-                            logic.parse_params([m
[31m-                                request.GET, ignore_keys=CACHE_PARAMETERS))))[m
[31m-                portal = LocalCKAN()[m
[31m-                patch_meta = {'id':package_id,'data_type':data['data_type']}[m
[31m-                package = portal.action.package_patch(**patch_meta)[m
[31m-                h.redirect_to(controller='dataset', action='read', id=package_id)[m
[31m-            except logic.ValidationError as e:[m
[31m-                return e[m
[31m-[m
[31m-class DatasetImportController(p.toolkit.BaseController):[m
[31m-[m
[31m-    def _record_type_process(self, data_dict):[m
[31m-        try:[m
[31m-            record_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp2_Meta_Record')[m
[31m-            record_df.drop(0, inplace=True)[m
[31m-            record_df["data_type"] = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô'[m
[31m-[m
[31m-            record_df.columns = ['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'][m
[31m-            record_df.drop(['d_type'], axis=1, inplace=True)[m
[31m-            record_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            record_df = record_df.astype('unicode')[m
[31m-            record_df = record_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-            record_df['high_value_dataset'] = np.where(record_df['high_value_dataset'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            record_df['reference_data'] = np.where(record_df['reference_data'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-[m
[31m-            record_df["dataset_name"] = record_df["name"][m
[31m-            record_df["name"] = record_df["name"].str.lower()[m
[31m-            record_df["name"].replace('\s', '-', regex=True, inplace=True)[m
[31m-            if data_dict['template_org'] != 'all':[m
[31m-                record_df = record_df.loc[record_df['owner_org'] == str(data_dict['template_org']).encode('utf-8')][m
[31m-                record_df.reset_index(drop=True, inplace=True)[m
[31m-            record_df["owner_org"] = data_dict['owner_org'][m
[31m-            record_df["private"] = True[m
[31m-            record_df["allow_harvest"] = "False"[m
[31m-            record_df['tag_string'] = record_df['tag_string'].str.split(',').apply(lambda x: [e.strip() for e in x]).tolist()[m
[31m-[m
[31m-            record_df["created_date"] = pd.to_datetime((pd.to_numeric(record_df["created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+record_df["created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            record_df["last_updated_date"] = pd.to_datetime((pd.to_numeric(record_df["last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+record_df["last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-[m
[31m-            objective_choices = [u'‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏£‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡πÅ‡∏ú‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 3 (‡∏°‡∏ï‡∏¥‡∏Ñ‡∏£‡∏°. 4 ‡∏ò.‡∏Ñ. 2560)',u'‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•/‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô',u'‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á',u'‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô',u'‡∏î‡∏±‡∏ä‡∏ô‡∏µ/‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            record_df['objective_other'] = record_df['objective'].isin(objective_choices)[m
[31m-            record_df['objective_other'] = np.where(record_df['objective_other'], 'True', record_df['objective'])[m
[31m-            record_df['objective'] = np.where(record_df['objective_other'] == 'True', record_df['objective'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['objective_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            update_frequency_unit_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', u'‡∏õ‡∏µ', u'‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏õ‡∏µ',u'‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™',u'‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',u'‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå',u'‡∏ß‡∏±‡∏ô',u'‡∏ß‡∏±‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£',u'‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á',u'‡∏ô‡∏≤‡∏ó‡∏µ',u'‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á',u'‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'][m
[31m-            record_df['update_frequency_unit_other'] = record_df['update_frequency_unit'].isin(update_frequency_unit_choices)[m
[31m-            record_df['update_frequency_unit_other'] = np.where(record_df['update_frequency_unit_other'], 'True', record_df['update_frequency_unit'])[m
[31m-            record_df['update_frequency_unit'] = np.where(record_df['update_frequency_unit_other'] == 'True', record_df['update_frequency_unit'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['update_frequency_unit_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            geo_coverage_choices = [u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÇ‡∏•‡∏Å', u'‡∏ó‡∏ß‡∏µ‡∏õ/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏ó‡∏ß‡∏µ‡∏õ',u'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',u'‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏†‡∏≤‡∏Ñ',u'‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',u'‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',u'‡∏ï‡∏≥‡∏ö‡∏•',u'‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô',u'‡πÄ‡∏ó‡∏®‡∏ö‡∏≤‡∏•/‡∏≠‡∏ö‡∏ï.',u'‡∏û‡∏¥‡∏Å‡∏±‡∏î',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            record_df['geo_coverage_other'] = record_df['geo_coverage'].isin(geo_coverage_choices)[m
[31m-            record_df['geo_coverage_other'] = np.where(record_df['geo_coverage_other'], 'True', record_df['geo_coverage'])[m
[31m-            record_df['geo_coverage'] = np.where(record_df['geo_coverage_other'] == 'True', record_df['geo_coverage'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['geo_coverage_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_format_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', 'Database', 'CSV','XML','Image','Video','Audio','Text','JSON','HTML','DOC/DOCX','XLS','PDF','RDF','NoSQL','Arc/Info Coverage','Shapefile','GeoTiff','GML'][m
[31m-            record_df['data_format_other'] = record_df['data_format'].isin(data_format_choices)[m
[31m-            record_df['data_format_other'] = np.where(record_df['data_format_other'], 'True', record_df['data_format'])[m
[31m-            record_df['data_format'] = np.where(record_df['data_format_other'] == 'True', record_df['data_format'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['data_format_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            license_id_choices = ['Open Data Common', 'Creative Commons Attributions','Creative Commons Attribution Non-Commercial','Creative Commons Attribution Share-Alike','Creative Commons Attribution Non-Commercial Share-Alike','Creative Commons Attribution No-Derivs','Creative Commons Attribution Non-Commercial No-Derivs'][m
[31m-            record_df['license_id_other'] = record_df['license_id'].isin(license_id_choices)[m
[31m-            record_df['license_id_other'] = np.where(record_df['license_id_other'], 'True', record_df['license_id'])[m
[31m-            record_df['license_id'] = np.where(record_df['license_id_other'] == 'True', record_df['license_id'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['license_id_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            data_support_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏ä‡∏ô',u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô/‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ò‡∏¥/‡∏™‡∏°‡∏≤‡∏Ñ‡∏°',u'‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤'][m
[31m-            record_df['data_support_other'] = record_df['data_support'].isin(data_support_choices)[m
[31m-            record_df['data_support_other'] = np.where(record_df['data_support_other'], 'True', record_df['data_support'])[m
[31m-            record_df['data_support'] = np.where(record_df['data_support_other'] == 'True', record_df['data_support'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['data_support_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_collect_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ',u'‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•', u'‡∏Ñ‡∏£‡∏±‡∏ß‡πÄ‡∏£‡∏∑‡∏≠‡∏ô/‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß', u'‡∏ö‡πâ‡∏≤‡∏ô/‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏≤‡∏®‡∏±‡∏¢',u'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏´‡πâ‡∏≤‡∏á‡∏£‡πâ‡∏≤‡∏ô/‡∏™‡∏ñ‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£',u'‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£/‡∏™‡∏¥‡πà‡∏á‡∏õ‡∏•‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á',u'‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏©‡∏ï‡∏£ ‡∏õ‡∏£‡∏∞‡∏°‡∏á ‡∏õ‡πà‡∏≤‡πÑ‡∏°‡πâ',u'‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏û‡∏∑‡∏ä',u'‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà',u'‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ô‡πâ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏°‡πà‡∏ô‡πâ‡∏≥ ‡∏≠‡πà‡∏≤‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏ô‡πâ‡∏≥',u'‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡∏ô‡∏ô ‡∏ó‡∏≤‡∏á‡∏£‡∏ñ‡πÑ‡∏ü',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            record_df['data_collect_other'] = record_df['data_collect'].isin(data_collect_choices)[m
[31m-            record_df['data_collect_other'] = np.where(record_df['data_collect_other'], 'True', record_df['data_collect'])[m
[31m-            record_df['data_collect'] = np.where(record_df['data_collect_other'] == 'True', record_df['data_collect'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['data_collect_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_language_choices = ['',u'‡πÑ‡∏ó‡∏¢', u'‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©', u'‡∏à‡∏µ‡∏ô',u'‡∏°‡∏•‡∏≤‡∏¢‡∏π',u'‡∏û‡∏°‡πà‡∏≤',u'‡∏•‡∏≤‡∏ß',u'‡πÄ‡∏Ç‡∏°‡∏£',u'‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô',u'‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ',u'‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™',u'‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏±‡∏ô',u'‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            record_df['data_language_other'] = record_df['data_language'].isin(data_language_choices)[m
[31m-            record_df['data_language_other'] = np.where(record_df['data_language_other'], 'True', record_df['data_language'])[m
[31m-            record_df['data_language'] = np.where(record_df['data_language_other'] == 'True', record_df['data_language'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['data_language_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            record_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            [m
[31m-        except Exception as err:[m
[31m-           log.info(err)[m
[31m-           record_df = pd.DataFrame(columns=['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'])[m
[31m-           record_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-           record_df = record_df.astype('unicode')[m
[31m-           record_df = record_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-            [m
[31m-        portal = LocalCKAN()[m
[31m-[m
[31m-        package_dict_list = record_df.to_dict('records')[m
[31m-        for pkg_meta in package_dict_list:[m
[31m-            try:[m
[31m-                if pkg_meta['data_language'] == '':[m
[31m-                    pkg_meta.pop('data_language', None)[m
[31m-                    pkg_meta.pop('data_language_other', None)[m
[31m-                package = portal.action.package_create(**pkg_meta)[m
[31m-                log_str = 'package_create: '+datetime.datetime.now().isoformat()+' -- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(package.get("name"))+' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n'[m
[31m-                activity_dict = {"data": {"actor": six.ensure_text(data_dict["importer"]), "package":package, [m
[31m-                    "import": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": package.get("id"), [m
[31m-                    "activity_type": "new package"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-                record_df.loc[record_df['name'] == pkg_meta['name'], 'success'] = '1'[m
[31m-            except Exception as err:[m
[31m-                record_df.loc[record_df['name'] == pkg_meta['name'], 'success'] = '0'[m
[31m-                log_str = 'package_error: '+datetime.datetime.now().isoformat()+' -- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(pkg_meta['name'])+' : '+str(err)+'\n'[m
[31m-                activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "activity_type": "changed user"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-[m
[31m-        try:[m
[31m-            resource_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp3_Resource_Record')[m
[31m-            resource_df.drop(0, inplace=True)[m
[31m-            resource_df.columns = ['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-        except:[m
[31m-            resource_df = pd.DataFrame(columns=['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'])[m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        try:[m
[31m-            final_df = pd.merge(record_df,resource_df,how='left',left_on='dataset_name',right_on='dataset_name')[m
[31m-            final_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = final_df[(final_df['resource_url'] != '') & (final_df['success'] == '1')][m
[31m-            resource_df = resource_df[['name','success','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect']][m
[31m-            resource_df.columns = ['package_id','success','name','url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df["resource_created_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_last_updated_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df['created'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df['last_modified'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            resource_dict_list = resource_df.to_dict('records')[m
[31m-[m
[31m-            for resource_dict in resource_dict_list:[m
[31m-                res_meta = resource_dict[m
[31m-                resource = portal.action.resource_create(**res_meta)[m
[31m-                log.info('resource_create: '+datetime.datetime.now().isoformat()+' -- '+str(resource)+'\n')[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-[m
[31m-    def _stat_type_process(self, data_dict):[m
[31m-        try:[m
[31m-            stat_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp2_Meta_Stat')[m
[31m-            stat_df.drop(0, inplace=True)[m
[31m-            stat_df["data_type"] = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥'[m
[31m-[m
[31m-            stat_df.columns = ['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','first_year_of_data','last_year_of_data','data_release_calendar','last_updated_date','disaggregate','unit_of_measure','unit_of_multiplier','calculation_method','standard','url','data_language','official_statistics','data_type'][m
[31m-            stat_df.drop(['d_type'], axis=1, inplace=True)[m
[31m-            stat_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            stat_df = stat_df.astype('unicode')[m
[31m-            stat_df = stat_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-            stat_df['official_statistics'] = np.where(stat_df['official_statistics'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            [m
[31m-            stat_df["dataset_name"] = stat_df["name"][m
[31m-            stat_df["name"] = stat_df["name"].str.lower()[m
[31m-            stat_df["name"].replace('\s', '-', regex=True, inplace=True)[m
[31m-            if data_dict['template_org'] != 'all':[m
[31m-                stat_df = stat_df.loc[stat_df['owner_org'] == str(data_dict['template_org']).encode('utf-8')][m
[31m-                stat_df.reset_index(drop=True, inplace=True)[m
[31m-            stat_df["owner_org"] = data_dict['owner_org'][m
[31m-            stat_df["private"] = True[m
[31m-            stat_df["allow_harvest"] = "False"[m
[31m-            stat_df['tag_string'] = stat_df['tag_string'].str.split(',').apply(lambda x: [e.strip() for e in x]).tolist()[m
[31m-[m
[31m-            stat_df["last_updated_date"] = pd.to_datetime((pd.to_numeric(stat_df["last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+stat_df["last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-[m
[31m-            objective_choices = [u'‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏£‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡πÅ‡∏ú‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 3 (‡∏°‡∏ï‡∏¥‡∏Ñ‡∏£‡∏°. 4 ‡∏ò.‡∏Ñ. 2560)',u'‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•/‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô',u'‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á',u'‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô',u'‡∏î‡∏±‡∏ä‡∏ô‡∏µ/‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            stat_df['objective_other'] = stat_df['objective'].isin(objective_choices)[m
[31m-            stat_df['objective_other'] = np.where(stat_df['objective_other'], 'True', stat_df['objective'])[m
[31m-            stat_df['objective'] = np.where(stat_df['objective_other'] == 'True', stat_df['objective'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['objective_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            update_frequency_unit_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', u'‡∏õ‡∏µ', u'‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏õ‡∏µ',u'‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™',u'‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',u'‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå',u'‡∏ß‡∏±‡∏ô',u'‡∏ß‡∏±‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£',u'‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á',u'‡∏ô‡∏≤‡∏ó‡∏µ',u'‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á',u'‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'][m
[31m-            stat_df['update_frequency_unit_other'] = stat_df['update_frequency_unit'].isin(update_frequency_unit_choices)[m
[31m-            stat_df['update_frequency_unit_other'] = np.where(stat_df['update_frequency_unit_other'], 'True', stat_df['update_frequency_unit'])[m
[31m-            stat_df['update_frequency_unit'] = np.where(stat_df['update_frequency_unit_other'] == 'True', stat_df['update_frequency_unit'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['update_frequency_unit_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            geo_coverage_choices = [u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÇ‡∏•‡∏Å', u'‡∏ó‡∏ß‡∏µ‡∏õ/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏ó‡∏ß‡∏µ‡∏õ',u'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',u'‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏†‡∏≤‡∏Ñ',u'‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',u'‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',u'‡∏ï‡∏≥‡∏ö‡∏•',u'‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô',u'‡πÄ‡∏ó‡∏®‡∏ö‡∏≤‡∏•/‡∏≠‡∏ö‡∏ï.',u'‡∏û‡∏¥‡∏Å‡∏±‡∏î',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            stat_df['geo_coverage_other'] = stat_df['geo_coverage'].isin(geo_coverage_choices)[m
[31m-            stat_df['geo_coverage_other'] = np.where(stat_df['geo_coverage_other'], 'True', stat_df['geo_coverage'])[m
[31m-            stat_df['geo_coverage'] = np.where(stat_df['geo_coverage_other'] == 'True', stat_df['geo_coverage'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['geo_coverage_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_format_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', 'Database', 'CSV','XML','Image','Video','Audio','Text','JSON','HTML','DOC/DOCX','XLS','PDF','RDF','NoSQL','Arc/Info Coverage','Shapefile','GeoTiff','GML'][m
[31m-            stat_df['data_format_other'] = stat_df['data_format'].isin(data_format_choices)[m
[31m-            stat_df['data_format_other'] = np.where(stat_df['data_format_other'], 'True', stat_df['data_format'])[m
[31m-            stat_df['data_format'] = np.where(stat_df['data_format_other'] == 'True', stat_df['data_format'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['data_format_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            license_id_choices = ['Open Data Common', 'Creative Commons Attributions','Creative Commons Attribution Non-Commercial','Creative Commons Attribution Share-Alike','Creative Commons Attribution Non-Commercial Share-Alike','Creative Commons Attribution No-Derivs','Creative Commons Attribution Non-Commercial No-Derivs'][m
[31m-            stat_df['license_id_other'] = stat_df['license_id'].isin(license_id_choices)[m
[31m-            stat_df['license_id_other'] = np.where(stat_df['license_id_other'], 'True', stat_df['license_id'])[m
[31m-            stat_df['license_id'] = np.where(stat_df['license_id_other'] == 'True', stat_df['license_id'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['license_id_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            stat_df["data_release_calendar"] = pd.to_datetime((pd.to_numeric(stat_df["data_release_calendar"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+stat_df["data_release_calendar"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            [m
[31m-            disaggregate_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÄ‡∏û‡∏®', u'‡∏≠‡∏≤‡∏¢‡∏∏/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏≤‡∏¢‡∏∏',u'‡∏™‡∏ñ‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡∏™‡∏°‡∏£‡∏™',u'‡∏®‡∏≤‡∏™‡∏ô‡∏≤',u'‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤',u'‡∏≠‡∏≤‡∏ä‡∏µ‡∏û',u'‡∏™‡∏ñ‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô',u'‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°/‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£',u'‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ',u'‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà',u'‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            stat_df['disaggregate_other'] = stat_df['disaggregate'].isin(disaggregate_choices)[m
[31m-            stat_df['disaggregate_other'] = np.where(stat_df['disaggregate_other'], 'True', stat_df['disaggregate'])[m
[31m-            stat_df['disaggregate'] = np.where(stat_df['disaggregate_other'] == 'True', stat_df['disaggregate'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['disaggregate_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            unit_of_multiplier_choices = ['',u'‡∏´‡∏ô‡πà‡∏ß‡∏¢', u'‡∏™‡∏¥‡∏ö', u'‡∏£‡πâ‡∏≠‡∏¢',u'‡∏û‡∏±‡∏ô',u'‡∏´‡∏°‡∏∑‡πà‡∏ô',u'‡πÅ‡∏™‡∏ô',u'‡∏•‡πâ‡∏≤‡∏ô',u'‡∏™‡∏¥‡∏ö‡∏•‡πâ‡∏≤‡∏ô',u'‡∏£‡πâ‡∏≠‡∏¢‡∏•‡πâ‡∏≤‡∏ô',u'‡∏û‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏ô',u'‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏•‡πâ‡∏≤‡∏ô',u'‡πÅ‡∏™‡∏ô‡∏•‡πâ‡∏≤‡∏ô',u'‡∏•‡πâ‡∏≤‡∏ô‡∏•‡πâ‡∏≤‡∏ô',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            stat_df['unit_of_multiplier_other'] = stat_df['unit_of_multiplier'].isin(unit_of_multiplier_choices)[m
[31m-            stat_df['unit_of_multiplier_other'] = np.where(stat_df['unit_of_multiplier_other'], 'True', stat_df['unit_of_multiplier'])[m
[31m-            stat_df['unit_of_multiplier'] = np.where(stat_df['unit_of_multiplier_other'] == 'True', stat_df['unit_of_multiplier'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['unit_of_multiplier_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            data_language_choices = ['',u'‡πÑ‡∏ó‡∏¢', u'‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©', u'‡∏à‡∏µ‡∏ô',u'‡∏°‡∏•‡∏≤‡∏¢‡∏π',u'‡∏û‡∏°‡πà‡∏≤',u'‡∏•‡∏≤‡∏ß',u'‡πÄ‡∏Ç‡∏°‡∏£',u'‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô',u'‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ',u'‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™',u'‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏±‡∏ô',u'‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            stat_df['data_language_other'] = stat_df['data_language'].isin(data_language_choices)[m
[31m-            stat_df['data_language_other'] = np.where(stat_df['data_language_other'], 'True', stat_df['data_language'])[m
[31m-            stat_df['data_language'] = np.where(stat_df['data_language_other'] == 'True', stat_df['data_language'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['data_language_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            stat_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            [m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-            stat_df = pd.DataFrame(columns=['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','first_year_of_data','last_year_of_data','data_release_calendar','last_updated_date','disaggregate','unit_of_measure','unit_of_multiplier','calculation_method','standard','url','data_language','official_statistics','data_type'])[m
[31m-            stat_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            stat_df = stat_df.astype('unicode')[m
[31m-            stat_df = stat_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        portal = LocalCKAN()[m
[31m-[m
[31m-        package_dict_list = stat_df.to_dict('records')[m
[31m-        for pkg_meta in package_dict_list:[m
[31m-            try:[m
[31m-                if pkg_meta['disaggregate'] == '':[m
[31m-                    pkg_meta.pop('disaggregate', None)[m
[31m-                    pkg_meta.pop('disaggregate_other', None)[m
[31m-                if pkg_meta['data_language'] == '':[m
[31m-                    pkg_meta.pop('data_language', None)[m
[31m-                    pkg_meta.pop('data_language_other', None)[m
[31m-                package = portal.action.package_create(**pkg_meta)[m
[31m-                log_str = 'package_create: '+datetime.datetime.now().isoformat()+' -- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(package.get("name"))+' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n'[m
[31m-                activity_dict = {"data": {"actor": six.ensure_text(data_dict["importer"]), "package":package, [m
[31m-                    "import": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": package.get("id"), [m
[31m-                    "activity_type": "new package"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-                stat_df.loc[stat_df['name'] == pkg_meta['name'], 'success'] = '1'[m
[31m-            except Exception as err:[m
[31m-                stat_df.loc[stat_df['name'] == pkg_meta['name'], 'success'] = '0'[m
[31m-                log_str = 'package_error: '+datetime.datetime.now().isoformat()+' -- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(pkg_meta['name'])+' : '+str(err)+'\n'[m
[31m-                activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "activity_type": "changed user"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-[m
[31m-        try:[m
[31m-            resource_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp3_Resource_Stat')[m
[31m-            resource_df.drop(0, inplace=True)[m
[31m-            resource_df.columns = ['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_first_year_of_data','resource_last_year_of_data','resource_data_release_calendar','resource_disaggregate','resource_unit_of_measure','resource_unit_of_multiplier','resource_official_statistics'][m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-        except:[m
[31m-            resource_df = pd.DataFrame(columns=['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_first_year_of_data','resource_last_year_of_data','resource_data_release_calendar','resource_disaggregate','resource_unit_of_measure','resource_unit_of_multiplier','resource_official_statistics'])[m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        try:[m
[31m-            final_df = pd.merge(stat_df,resource_df,how='left',left_on='dataset_name',right_on='dataset_name')[m
[31m-            final_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = final_df[(final_df['resource_url'] != '') & (final_df['success'] == '1')][m
[31m-            resource_df = resource_df[['name','success','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_first_year_of_data','resource_last_year_of_data','resource_data_release_calendar','resource_disaggregate','resource_unit_of_measure','resource_unit_of_multiplier','resource_official_statistics']][m
[31m-            resource_df.columns = ['package_id','success','name','url','description','resource_accessible_condition','resource_last_updated_date','format','resource_first_year_of_data','resource_last_year_of_data','resource_data_release_calendar','resource_disaggregate','resource_unit_of_measure','resource_unit_of_multiplier','resource_official_statistics'][m
[31m-[m
[31m-            disaggregate_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÄ‡∏û‡∏®', u'‡∏≠‡∏≤‡∏¢‡∏∏/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏≤‡∏¢‡∏∏',u'‡∏™‡∏ñ‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡∏™‡∏°‡∏£‡∏™',u'‡∏®‡∏≤‡∏™‡∏ô‡∏≤',u'‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤',u'‡∏≠‡∏≤‡∏ä‡∏µ‡∏û',u'‡∏™‡∏ñ‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô',u'‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°/‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£',u'‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ',u'‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà',u'‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            resource_df['resource_disaggregate_other'] = resource_df['resource_disaggregate'].isin(disaggregate_choices)[m
[31m-            resource_df['resource_disaggregate_other'] = np.where(resource_df['resource_disaggregate_other'], 'True', resource_df['resource_disaggregate'])[m
[31m-            resource_df['resource_disaggregate'] = np.where(resource_df['resource_disaggregate_other'] == 'True', resource_df['resource_disaggregate'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            resource_df['resource_disaggregate_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            resource_df["resource_data_release_calendar"] = pd.to_datetime((pd.to_numeric(resource_df["resource_data_release_calendar"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_data_release_calendar"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_last_updated_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df['created'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df['last_modified'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            resource_dict_list = resource_df.to_dict('records')[m
[31m-[m
[31m-            for resource_dict in resource_dict_list:[m
[31m-                res_meta = resource_dict[m
[31m-                if res_meta['resource_disaggregate'] == '':[m
[31m-                    res_meta.pop('resource_disaggregate', None)[m
[31m-                    res_meta.pop('resource_disaggregate_other', None)[m
[31m-                resource = portal.action.resource_create(**res_meta)[m
[31m-                log.info('resource_create: '+datetime.datetime.now().isoformat()+' -- '+str(resource)+'\n')[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-[m
[31m-    def _gis_type_process(self, data_dict):[m
[31m-        try:[m
[31m-            gis_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp2_Meta_GIS')[m
[31m-            gis_df.drop(0, inplace=True)[m
[31m-            gis_df["data_type"] = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏π‡∏°‡∏¥‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà'[m
[31m-[m
[31m-            gis_df.columns = ['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','geographic_data_set','equivalent_scale','west_bound_longitude','east_bound_longitude','north_bound_longitude','south_bound_longitude','positional_accuracy','reference_period','last_updated_date','data_release_calendar','data_release_date','url','data_language','data_type'][m
[31m-            gis_df.drop(['d_type'], axis=1, inplace=True)[m
[31m-            gis_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            gis_df = gis_df.astype('unicode')[m
[31m-            gis_df = gis_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-            gis_df["dataset_name"] = gis_df["name"][m
[31m-            gis_df["name"] = gis_df["name"].str.lower()[m
[31m-            gis_df["name"].replace('\s', '-', regex=True, inplace=True)[m
[31m-            if data_dict['template_org'] != 'all':[m
[31m-                gis_df = gis_df.loc[gis_df['owner_org'] == str(data_dict['template_org']).encode('utf-8')][m
[31m-                gis_df.reset_index(drop=True, inplace=True)[m
[31m-            gis_df["owner_org"] = data_dict['owner_org'][m
[31m-            gis_df["private"] = True[m
[31m-            gis_df["allow_harvest"] = "False"[m
[31m-            gis_df['tag_string'] = gis_df['tag_string'].str.split(',').apply(lambda x: [e.strip() for e in x]).tolist()[m
[31m-[m
[31m-            gis_df["last_updated_date"] = pd.to_datetime((pd.to_numeric(gis_df["last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+gis_df["last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-[m
[31m-            objective_choices = [u'‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏£‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡πÅ‡∏ú‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 3 (‡∏°‡∏ï‡∏¥‡∏Ñ‡∏£‡∏°. 4 ‡∏ò.‡∏Ñ. 2560)',u'‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•/‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô',u'‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á',u'‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô',u'‡∏î‡∏±‡∏ä‡∏ô‡∏µ/‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            gis_df['objective_other'] = gis_df['objective'].isin(objective_choices)[m
[31m-            gis_df['objective_other'] = np.where(gis_df['objective_other'], 'True', gis_df['objective'])[m
[31m-            gis_df['objective'] = np.where(gis_df['objective_other'] == 'True', gis_df['objective'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['objective_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            update_frequency_unit_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', u'‡∏õ‡∏µ', u'‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏õ‡∏µ',u'‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™',u'‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',u'‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå',u'‡∏ß‡∏±‡∏ô',u'‡∏ß‡∏±‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£',u'‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á',u'‡∏ô‡∏≤‡∏ó‡∏µ',u'‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á',u'‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'][m
[31m-            gis_df['update_frequency_unit_other'] = gis_df['update_frequency_unit'].isin(update_frequency_unit_choices)[m
[31m-            gis_df['update_frequency_unit_other'] = np.where(gis_df['update_frequency_unit_other'], 'True', gis_df['update_frequency_unit'])[m
[31m-            gis_df['update_frequency_unit'] = np.where(gis_df['update_frequency_unit_other'] == 'True', gis_df['update_frequency_unit'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['update_frequency_unit_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            geo_coverage_choices = [u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÇ‡∏•‡∏Å', u'‡∏ó‡∏ß‡∏µ‡∏õ/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏ó‡∏ß‡∏µ‡∏õ',u'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',u'‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏†‡∏≤‡∏Ñ',u'‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',u'‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',u'‡∏ï‡∏≥‡∏ö‡∏•',u'‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô',u'‡πÄ‡∏ó‡∏®‡∏ö‡∏≤‡∏•/‡∏≠‡∏ö‡∏ï.',u'‡∏û‡∏¥‡∏Å‡∏±‡∏î',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            gis_df['geo_coverage_other'] = gis_df['geo_coverage'].isin(geo_coverage_choices)[m
[31m-            gis_df['geo_coverage_other'] = np.where(gis_df['geo_coverage_other'], 'True', gis_df['geo_coverage'])[m
[31m-            gis_df['geo_coverage'] = np.where(gis_df['geo_coverage_other'] == 'True', gis_df['geo_coverage'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['geo_coverage_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_format_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', 'Database', 'CSV','XML','Image','Video','Audio','Text','JSON','HTML','DOC/DOCX','XLS','PDF','RDF','NoSQL','Arc/Info Coverage','Shapefile','GeoTiff','GML'][m
[31m-            gis_df['data_format_other'] = gis_df['data_format'].isin(data_format_choices)[m
[31m-            gis_df['data_format_other'] = np.where(gis_df['data_format_other'], 'True', gis_df['data_format'])[m
[31m-            gis_df['data_format'] = np.where(gis_df['data_format_other'] == 'True', gis_df['data_format'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['data_format_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            license_id_choices = ['Open Data Common', 'Creative Commons Attributions','Creative Commons Attribution Non-Commercial','Creative Commons Attribution Share-Alike','Creative Commons Attribution Non-Commercial Share-Alike','Creative Commons Attribution No-Derivs','Creative Commons Attribution Non-Commercial No-Derivs'][m
[31m-            gis_df['license_id_other'] = gis_df['license_id'].isin(license_id_choices)[m
[31m-            gis_df['license_id_other'] = np.where(gis_df['license_id_other'], 'True', gis_df['license_id'])[m
[31m-            gis_df['license_id'] = np.where(gis_df['license_id_other'] == 'True', gis_df['license_id'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['license_id_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            equivalent_scale_choices = ['','1:4,000', '1:10,000', '1:25,000','1:50,000','1:250,000'][m
[31m-            gis_df['equivalent_scale_other'] = gis_df['equivalent_scale'].isin(equivalent_scale_choices)[m
[31m-            gis_df['equivalent_scale_other'] = np.where(gis_df['equivalent_scale_other'], 'True', gis_df['equivalent_scale'])[m
[31m-            gis_df['equivalent_scale'] = np.where(gis_df['equivalent_scale_other'] == 'True', gis_df['equivalent_scale'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['equivalent_scale_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            gis_df["data_release_calendar"] = pd.to_datetime((pd.to_numeric(gis_df["data_release_calendar"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+gis_df["data_release_calendar"].str.slice(start=4),errors='coerce').astype(str)[m
[31m-            gis_df["data_release_date"] = pd.to_datetime((pd.to_numeric(gis_df["data_release_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+gis_df["data_release_date"].str.slice(start=4),errors='coerce').astype(str)[m
[31m-[m
[31m-            data_language_choices = ['',u'‡πÑ‡∏ó‡∏¢', u'‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©', u'‡∏à‡∏µ‡∏ô',u'‡∏°‡∏•‡∏≤‡∏¢‡∏π',u'‡∏û‡∏°‡πà‡∏≤',u'‡∏•‡∏≤‡∏ß',u'‡πÄ‡∏Ç‡∏°‡∏£',u'‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô',u'‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ',u'‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™',u'‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏±‡∏ô',u'‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            gis_df['data_language_other'] = gis_df['data_language'].isin(data_language_choices)[m
[31m-            gis_df['data_language_other'] = np.where(gis_df['data_language_other'], 'True', gis_df['data_language'])[m
[31m-            gis_df['data_language'] = np.where(gis_df['data_language_other'] == 'True', gis_df['data_language'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['data_language_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            gis_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-            gis_df = pd.DataFrame(columns=['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','geographic_data_set','equivalent_scale','west_bound_longitude','east_bound_longitude','north_bound_longitude','south_bound_longitude','positional_accuracy','reference_period','last_updated_date','data_release_calendar','data_release_date','url','data_language','data_type'])[m
[31m-            gis_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            gis_df = gis_df.astype('unicode')[m
[31m-            gis_df = gis_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        portal = LocalCKAN()[m
[31m-[m
[31m-        package_dict_list = gis_df.to_dict('records')[m
[31m-        for pkg_meta in package_dict_list:[m
[31m-            try:[m
[31m-                if pkg_meta['data_language'] == '':[m
[31m-                    pkg_meta.pop('data_language', None)[m
[31m-                    pkg_meta.pop('data_language_other', None)[m
[31m-                package = portal.action.package_create(**pkg_meta)[m
[31m-                log_str = 'package_create: '+datetime.datetime.now().isoformat()+' -- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(package.get("name"))+' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n'[m
[31m-                activity_dict = {"data": {"actor": six.ensure_text(data_dict["importer"]), "package":package, [m
[31m-                    "import": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": package.get("id"), [m
[31m-                    "activity_type": "new package"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-                gis_df.loc[gis_df['name'] == pkg_meta['name'], 'success'] = '1'[m
[31m-            except Exception as err:[m
[31m-                gis_df.loc[gis_df['name'] == pkg_meta['name'], 'success'] = '0'[m
[31m-                log_str = 'package_error: '+datetime.datetime.now().isoformat()+' -- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(pkg_meta['name'])+' : '+str(err)+'\n'[m
[31m-                activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "activity_type": "changed user"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-[m
[31m-        try:[m
[31m-            resource_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp3_Resource_GIS')[m
[31m-            resource_df.drop(0, inplace=True)[m
[31m-            resource_df.columns = ['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_equivalent_scale','resource_geographic_data_set','resource_created_date','resource_data_release_date','resource_positional_accuracy'][m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-        except:[m
[31m-            resource_df = pd.DataFrame(columns=['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_equivalent_scale','resource_geographic_data_set','resource_created_date','resource_data_release_date','resource_positional_accuracy'])[m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        try:[m
[31m-            final_df = pd.merge(gis_df,resource_df,how='left',left_on='dataset_name',right_on='dataset_name')[m
[31m-            final_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = final_df[(final_df['resource_url'] != '') & (final_df['success'] == '1')][m
[31m-            resource_df = resource_df[['name','success','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_equivalent_scale','resource_geographic_data_set','resource_created_date','resource_data_release_date','resource_positional_accuracy']][m
[31m-            resource_df.columns = ['package_id','success','name','url','description','resource_accessible_condition','resource_last_updated_date','format','resource_equivalent_scale','resource_geographic_data_set','resource_created_date','resource_data_release_date','resource_positional_accuracy'][m
[31m-            resource_df["resource_created_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_last_updated_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_data_release_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_data_release_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_data_release_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df['created'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df['last_modified'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            resource_dict_list = resource_df.to_dict('records')[m
[31m-[m
[31m-            for resource_dict in resource_dict_list:[m
[31m-                res_meta = resource_dict[m
[31m-                resource = portal.action.resource_create(**res_meta)[m
[31m-                log.info('resource_create: '+datetime.datetime.now().isoformat()+' -- '+str(resource)+'\n')[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-[m
[31m-    def _multi_type_process(self, data_dict):[m
[31m-        try:[m
[31m-            multi_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp2_Meta_Multi')[m
[31m-            multi_df.drop(0, inplace=True)[m
[31m-            multi_df["data_type"] = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó'[m
[31m-[m
[31m-            multi_df.columns = ['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'][m
[31m-            multi_df.drop(['d_type'], axis=1, inplace=True)[m
[31m-            multi_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            multi_df = multi_df.astype('unicode')[m
[31m-            multi_df = multi_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-            multi_df['high_value_dataset'] = np.where(multi_df['high_value_dataset'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            multi_df['reference_data'] = np.where(multi_df['reference_data'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            [m
[31m-            multi_df["dataset_name"] = multi_df["name"][m
[31m-            multi_df["name"] = multi_df["name"].str.lower()[m
[31m-            multi_df["name"].replace('\s', '-', regex=True, inplace=True)[m
[31m-            if data_dict['template_org'] != 'all':[m
[31m-                multi_df = multi_df.loc[multi_df['owner_org'] == str(data_dict['template_org']).encode('utf-8')][m
[31m-                multi_df.reset_index(drop=True, inplace=True)[m
[31m-            multi_df["owner_org"] = data_dict['owner_org'][m
[31m-            multi_df["private"] = True[m
[31m-            multi_df["allow_harvest"] = "False"[m
[31m-            multi_df['tag_string'] = multi_df['tag_string'].str.split(',').apply(lambda x: [e.strip() for e in x]).tolist()[m
[31m-[m
[31m-            multi_df["created_date"] = pd.to_datetime((pd.to_numeric(multi_df["created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+multi_df["created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            multi_df["last_updated_date"] = pd.to_datetime((pd.to_numeric(multi_df["last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+multi_df["last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-[m
[31m-            objective_choices = [u'‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏£‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡πÅ‡∏ú‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 3 (‡∏°‡∏ï‡∏¥‡∏Ñ‡∏£‡∏°. 4 ‡∏ò.‡∏Ñ. 2560)',u'‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•/‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô',u'‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á',u'‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô',u'‡∏î‡∏±‡∏ä‡∏ô‡∏µ/‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            multi_df['objective_other'] = multi_df['objective'].isin(objective_choices)[m
[31m-            multi_df['objective_other'] = np.where(multi_df['objective_other'], 'True', multi_df['objective'])[m
[31m-            multi_df['objective'] = np.where(multi_df['objective_other'] == 'True', multi_df['objective'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['objective_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            update_frequency_unit_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', u'‡∏õ‡∏µ', u'‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏õ‡∏µ',u'‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™',u'‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',u'‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå',u'‡∏ß‡∏±‡∏ô',u'‡∏ß‡∏±‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£',u'‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á',u'‡∏ô‡∏≤‡∏ó‡∏µ',u'‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á',u'‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'][m
[31m-            multi_df['update_frequency_unit_other'] = multi_df['update_frequency_unit'].isin(update_frequency_unit_choices)[m
[31m-            multi_df['update_frequency_unit_other'] = np.where(multi_df['update_frequency_unit_other'], 'True', multi_df['update_frequency_unit'])[m
[31m-            multi_df['update_frequency_unit'] = np.where(multi_df['update_frequency_unit_other'] == 'True', multi_df['update_frequency_unit'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['update_frequency_unit_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            geo_coverage_choices = [u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÇ‡∏•‡∏Å', u'‡∏ó‡∏ß‡∏µ‡∏õ/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏ó‡∏ß‡∏µ‡∏õ',u'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',u'‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏†‡∏≤‡∏Ñ',u'‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',u'‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',u'‡∏ï‡∏≥‡∏ö‡∏•',u'‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô',u'‡πÄ‡∏ó‡∏®‡∏ö‡∏≤‡∏•/‡∏≠‡∏ö‡∏ï.',u'‡∏û‡∏¥‡∏Å‡∏±‡∏î',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            multi_df['geo_coverage_other'] = multi_df['geo_coverage'].isin(geo_coverage_choices)[m
[31m-            multi_df['geo_coverage_other'] = np.where(multi_df['geo_coverage_other'], 'True', multi_df['geo_coverage'])[m
[31m-            multi_df['geo_coverage'] = np.where(multi_df['geo_coverage_other'] == 'True', multi_df['geo_coverage'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['geo_coverage_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_format_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', 'Database', 'CSV','XML','Image','Video','Audio','Text','JSON','HTML','DOC/DOCX','XLS','PDF','RDF','NoSQL','Arc/Info Coverage','Shapefile','GeoTiff','GML'][m
[31m-            multi_df['data_format_other'] = multi_df['data_format'].isin(data_format_choices)[m
[31m-            multi_df['data_format_other'] = np.where(multi_df['data_format_other'], 'True', multi_df['data_format'])[m
[31m-            multi_df['data_format'] = np.where(multi_df['data_format_other'] == 'True', multi_df['data_format'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['data_format_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            license_id_choices = ['Open Data Common', 'Creative Commons Attributions','Creative Commons Attribution Non-Commercial','Creative Commons Attribution Share-Alike','Creative Commons Attribution Non-Commercial Share-Alike','Creative Commons Attribution No-Derivs','Creative Commons Attribution Non-Commercial No-Derivs'][m
[31m-            multi_df['license_id_other'] = multi_df['license_id'].isin(license_id_choices)[m
[31m-            multi_df['license_id_other'] = np.where(multi_df['license_id_other'], 'True', multi_df['license_id'])[m
[31m-            multi_df['license_id'] = np.where(multi_df['license_id_other'] == 'True', multi_df['license_id'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['license_id_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            data_support_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏ä‡∏ô',u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô/‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ò‡∏¥/‡∏™‡∏°‡∏≤‡∏Ñ‡∏°',u'‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤'][m
[31m-            multi_df['data_support_other'] = multi_df['data_support'].isin(data_support_choices)[m
[31m-            multi_df['data_support_other'] = np.where(multi_df['data_support_other'], 'True', multi_df['data_support'])[m
[31m-            multi_df['data_support'] = np.where(multi_df['data_support_other'] == 'True', multi_df['data_support'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['data_support_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_collect_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ',u'‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•', u'‡∏Ñ‡∏£‡∏±‡∏ß‡πÄ‡∏£‡∏∑‡∏≠‡∏ô/‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß', u'‡∏ö‡πâ‡∏≤‡∏ô/‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏≤‡∏®‡∏±‡∏¢',u'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏´‡πâ‡∏≤‡∏á‡∏£‡πâ‡∏≤‡∏ô/‡∏™‡∏ñ‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£',u'‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£/‡∏™‡∏¥‡πà‡∏á‡∏õ‡∏•‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á',u'‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏©‡∏ï‡∏£ ‡∏õ‡∏£‡∏∞‡∏°‡∏á ‡∏õ‡πà‡∏≤‡πÑ‡∏°‡πâ',u'‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏û‡∏∑‡∏ä',u'‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà',u'‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ô‡πâ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏°‡πà‡∏ô‡πâ‡∏≥ ‡∏≠‡πà‡∏≤‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏ô‡πâ‡∏≥',u'‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡∏ô‡∏ô ‡∏ó‡∏≤‡∏á‡∏£‡∏ñ‡πÑ‡∏ü',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            multi_df['data_collect_other'] = multi_df['data_collect'].isin(data_collect_choices)[m
[31m-            multi_df['data_collect_other'] = np.where(multi_df['data_collect_other'], 'True', multi_df['data_collect'])[m
[31m-            multi_df['data_collect'] = np.where(multi_df['data_collect_other'] == 'True', multi_df['data_collect'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['data_collect_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_language_choices = ['',u'‡πÑ‡∏ó‡∏¢', u'‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©', u'‡∏à‡∏µ‡∏ô',u'‡∏°‡∏•‡∏≤‡∏¢‡∏π',u'‡∏û‡∏°‡πà‡∏≤',u'‡∏•‡∏≤‡∏ß',u'‡πÄ‡∏Ç‡∏°‡∏£',u'‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô',u'‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ',u'‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™',u'‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏±‡∏ô',u'‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            multi_df['data_language_other'] = multi_df['data_language'].isin(data_language_choices)[m
[31m-            multi_df['data_language_other'] = np.where(multi_df['data_language_other'], 'True', multi_df['data_language'])[m
[31m-            multi_df['data_language'] = np.where(multi_df['data_language_other'] == 'True', multi_df['data_language'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['data_language_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            multi_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            [m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-            multi_df = pd.DataFrame(columns=['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'])[m
[31m-            multi_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            multi_df = multi_df.astype('unicode')[m
[31m-            multi_df = multi_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-            [m
[31m-        portal = LocalCKAN()[m
[31m-[m
[31m-        package_dict_list = multi_df.to_dict('records')[m
[31m-        for pkg_meta in package_dict_list:[m
[31m-            try:[m
[31m-                if pkg_meta['data_language'] == '':[m
[31m-                    pkg_meta.pop('data_language', None)[m
[31m-                    pkg_meta.pop('data_language_other', None)[m
[31m-                package = portal.action.package_create(**pkg_meta)[m
[31m-                log_str = 'package_create: '+datetime.datetime.now().isoformat()+' -- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(package.get("name"))+' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n'[m
[31m-                activity_dict = {"data": {"actor": six.ensure_text(data_dict["importer"]), "package":package, [m
[31m-                    "import": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": package.get("id"), [m
[31m-                    "activity_type": "new package"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-                multi_df.loc[multi_df['name'] == pkg_meta['name'], 'success'] = '1'[m
[31m-            except Exception as err:[m
[31m-                multi_df.loc[multi_df['name'] == pkg_meta['name'], 'success'] = '0'[m
[31m-                log_str = 'package_error: '+datetime.datetime.now().isoformat()+' -- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(pkg_meta['name'])+' : '+str(err)+'\n'[m
[31m-                activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "activity_type": "changed user"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-[m
[31m-        try:[m
[31m-            resource_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp3_Resource_Multi')[m
[31m-            resource_df.drop(0, inplace=True)[m
[31m-            resource_df.columns = ['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-        except:[m
[31m-            resource_df = pd.DataFrame(columns=['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'])[m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        try:[m
[31m-            final_df = pd.merge(multi_df,resource_df,how='left',left_on='dataset_name',right_on='dataset_name')[m
[31m-            final_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = final_df[(final_df['resource_url'] != '') & (final_df['success'] == '1')][m
[31m-            resource_df = resource_df[['name','success','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect']][m
[31m-            resource_df.columns = ['package_id','success','name','url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df["resource_created_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_last_updated_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df['created'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df['last_modified'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            resource_dict_list = resource_df.to_dict('records')[m
[31m-[m
[31m-            for resource_dict in resource_dict_list:[m
[31m-                res_meta = resource_dict[m
[31m-                resource = portal.action.resource_create(**res_meta)[m
[31m-                log.info('resource_create: '+datetime.datetime.now().isoformat()+' -- '+str(resource)+'\n')[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-[m
[31m-    def _other_type_process(self, data_dict):[m
[31m-        try:[m
[31m-            other_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp2_Meta_Other')[m
[31m-            other_df.drop(0, inplace=True)[m
[31m-            other_df["data_type"] = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏≠‡∏∑‡πà‡∏ô‡πÜ'[m
[31m-[m
[31m-            other_df.columns = ['name','data_type_other','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'][m
[31m-            other_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            other_df = other_df.astype('unicode')[m
[31m-            other_df = other_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-            other_df['high_value_dataset'] = np.where(other_df['high_value_dataset'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            other_df['reference_data'] = np.where(other_df['reference_data'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            [m
[31m-            other_df["dataset_name"] = other_df["name"][m
[31m-            other_df["name"] = other_df["name"].str.lower()[m
[31m-            other_df["name"].replace('\s', '-', regex=True, inplace=True)[m
[31m-            if data_dict['template_org'] != 'all':[m
[31m-                other_df = other_df.loc[other_df['owner_org'] == str(data_dict['template_org']).encode('utf-8')][m
[31m-                other_df.reset_index(drop=True, inplace=True)[m
[31m-            other_df["owner_org"] = data_dict['owner_org'][m
[31m-            other_df["private"] = True[m
[31m-            other_df["allow_harvest"] = "False"[m
[31m-            other_df['tag_string'] = other_df['tag_string'].str.split(',').apply(lambda x: [e.strip() for e in x]).tolist()[m
[31m-[m
[31m-            other_df["created_date"] = pd.to_datetime((pd.to_numeric(other_df["created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+other_df["created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            other_df["last_updated_date"] = pd.to_datetime((pd.to_numeric(other_df["last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+other_df["last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-[m
[31m-            objective_choices = [u'‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏£‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡πÅ‡∏ú‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 3 (‡∏°‡∏ï‡∏¥‡∏Ñ‡∏£‡∏°. 4 ‡∏ò.‡∏Ñ. 2560)',u'‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•/‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô',u'‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á',u'‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô',u'‡∏î‡∏±‡∏ä‡∏ô‡∏µ/‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            other_df['objective_other'] = other_df['objective'].isin(objective_choices)[m
[31m-            other_df['objective_other'] = np.where(other_df['objective_other'], 'True', other_df['objective'])[m
[31m-            other_df['objective'] = np.where(other_df['objective_other'] == 'True', other_df['objective'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['objective_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            update_frequency_unit_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', u'‡∏õ‡∏µ', u'‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏õ‡∏µ',u'‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™',u'‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',u'‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå',u'‡∏ß‡∏±‡∏ô',u'‡∏ß‡∏±‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£',u'‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á',u'‡∏ô‡∏≤‡∏ó‡∏µ',u'‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á',u'‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'][m
[31m-            other_df['update_frequency_unit_other'] = other_df['update_frequency_unit'].isin(update_frequency_unit_choices)[m
[31m-            other_df['update_frequency_unit_other'] = np.where(other_df['update_frequency_unit_other'], 'True', other_df['update_frequency_unit'])[m
[31m-            other_df['update_frequency_unit'] = np.where(other_df['update_frequency_unit_other'] == 'True', other_df['update_frequency_unit'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['update_frequency_unit_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            geo_coverage_choices = [u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÇ‡∏•‡∏Å', u'‡∏ó‡∏ß‡∏µ‡∏õ/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏ó‡∏ß‡∏µ‡∏õ',u'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',u'‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏†‡∏≤‡∏Ñ',u'‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',u'‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',u'‡∏ï‡∏≥‡∏ö‡∏•',u'‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô',u'‡πÄ‡∏ó‡∏®‡∏ö‡∏≤‡∏•/‡∏≠‡∏ö‡∏ï.',u'‡∏û‡∏¥‡∏Å‡∏±‡∏î',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            other_df['geo_coverage_other'] = other_df['geo_coverage'].isin(geo_coverage_choices)[m
[31m-            other_df['geo_coverage_other'] = np.where(other_df['geo_coverage_other'], 'True', other_df['geo_coverage'])[m
[31m-            other_df['geo_coverage'] = np.where(other_df['geo_coverage_other'] == 'True', other_df['geo_coverage'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['geo_coverage_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_format_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', 'Database', 'CSV','XML','Image','Video','Audio','Text','JSON','HTML','DOC/DOCX','XLS','PDF','RDF','NoSQL','Arc/Info Coverage','Shapefile','GeoTiff','GML'][m
[31m-            other_df['data_format_other'] = other_df['data_format'].isin(data_format_choices)[m
[31m-            other_df['data_format_other'] = np.where(other_df['data_format_other'], 'True', other_df['data_format'])[m
[31m-            other_df['data_format'] = np.where(other_df['data_format_other'] == 'True', other_df['data_format'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['data_format_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            license_id_choices = ['Open Data Common', 'Creative Commons Attributions','Creative Commons Attribution Non-Commercial','Creative Commons Attribution Share-Alike','Creative Commons Attribution Non-Commercial Share-Alike','Creative Commons Attribution No-Derivs','Creative Commons Attribution Non-Commercial No-Derivs'][m
[31m-            other_df['license_id_other'] = other_df['license_id'].isin(license_id_choices)[m
[31m-            other_df['license_id_other'] = np.where(other_df['license_id_other'], 'True', other_df['license_id'])[m
[31m-            other_df['license_id'] = np.where(other_df['license_id_other'] == 'True', other_df['license_id'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['license_id_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            data_support_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏ä‡∏ô',u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô/‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ò‡∏¥/‡∏™‡∏°‡∏≤‡∏Ñ‡∏°',u'‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤'][m
[31m-            other_df['data_support_other'] = other_df['data_support'].isin(data_support_choices)[m
[31m-            other_df['data_support_other'] = np.where(other_df['data_support_other'], 'True', other_df['data_support'])[m
[31m-            other_df['data_support'] = np.where(other_df['data_support_other'] == 'True', other_df['data_support'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['data_support_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_collect_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ',u'‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•', u'‡∏Ñ‡∏£‡∏±‡∏ß‡πÄ‡∏£‡∏∑‡∏≠‡∏ô/‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß', u'‡∏ö‡πâ‡∏≤‡∏ô/‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏≤‡∏®‡∏±‡∏¢',u'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏´‡πâ‡∏≤‡∏á‡∏£‡πâ‡∏≤‡∏ô/‡∏™‡∏ñ‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£',u'‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£/‡∏™‡∏¥‡πà‡∏á‡∏õ‡∏•‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á',u'‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏©‡∏ï‡∏£ ‡∏õ‡∏£‡∏∞‡∏°‡∏á ‡∏õ‡πà‡∏≤‡πÑ‡∏°‡πâ',u'‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏û‡∏∑‡∏ä',u'‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà',u'‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ô‡πâ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏°‡πà‡∏ô‡πâ‡∏≥ ‡∏≠‡πà‡∏≤‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏ô‡πâ‡∏≥',u'‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡∏ô‡∏ô ‡∏ó‡∏≤‡∏á‡∏£‡∏ñ‡πÑ‡∏ü',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            other_df['data_collect_other'] = other_df['data_collect'].isin(data_collect_choices)[m
[31m-            other_df['data_collect_other'] = np.where(other_df['data_collect_other'], 'True', other_df['data_collect'])[m
[31m-            other_df['data_collect'] = np.where(other_df['data_collect_other'] == 'True', other_df['data_collect'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['data_collect_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_language_choices = ['',u'‡πÑ‡∏ó‡∏¢', u'‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©', u'‡∏à‡∏µ‡∏ô',u'‡∏°‡∏•‡∏≤‡∏¢‡∏π',u'‡∏û‡∏°‡πà‡∏≤',u'‡∏•‡∏≤‡∏ß',u'‡πÄ‡∏Ç‡∏°‡∏£',u'‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô',u'‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ',u'‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™',u'‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏±‡∏ô',u'‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            other_df['data_language_other'] = other_df['data_language'].isin(data_language_choices)[m
[31m-            other_df['data_language_other'] = np.where(other_df['data_language_other'], 'True', other_df['data_language'])[m
[31m-            other_df['data_language'] = np.where(other_df['data_language_other'] == 'True', other_df['data_language'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['data_language_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            other_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            [m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-            other_df = pd.DataFrame(columns=['name','data_type_other','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'])[m
[31m-            other_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            other_df = other_df.astype('unicode')[m
[31m-            other_df = other_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        portal = LocalCKAN()[m
[31m-[m
[31m-        package_dict_list = other_df.to_dict('records')[m
[31m-        for pkg_meta in package_dict_list:[m
[31m-            try:[m
[31m-                if pkg_meta['data_language'] == '':[m
[31m-                    pkg_meta.pop('data_language', None)[m
[31m-                    pkg_meta.pop('data_language_other', None)[m
[31m-                package = portal.action.package_create(**pkg_meta)[m
[31m-                log_str = 'package_create: '+datetime.datetime.now().isoformat()+' -- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(package.get("name"))+' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n'[m
[31m-                activity_dict = {"data": {"actor": six.ensure_text(data_dict["importer"]), "package":package, [m
[31m-                    "import": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": package.get("id"), [m
[31m-                    "activity_type": "new package"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-                other_df.loc[other_df['name'] == pkg_meta['name'], 'success'] = '1'[m
[31m-            except Exception as err:[m
[31m-                other_df.loc[other_df['name'] == pkg_meta['name'], 'success'] = '0'[m
[31m-                log_str = 'package_error: '+datetime.datetime.now().isoformat()+' -- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(pkg_meta['name'])+' : '+str(err)+'\n'[m
[31m-                activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "activity_type": "changed user"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-[m
[31m-        try:[m
[31m-            resource_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp3_Resource_Other')[m
[31m-            resource_df.drop(0, inplace=True)[m
[31m-            resource_df.columns = ['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-        except:[m
[31m-            resource_df = pd.DataFrame(columns=['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'])[m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        try:[m
[31m-            final_df = pd.merge(other_df,resource_df,how='left',left_on='dataset_name',right_on='dataset_name')[m
[31m-            final_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = final_df[(final_df['resource_url'] != '') & (final_df['success'] == '1')][m
[31m-            resource_df = resource_df[['name','success','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect']][m
[31m-            resource_df.columns = ['package_id','success','name','url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df["resource_created_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_last_updated_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df['created'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df['last_modified'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            resource_dict_list = resource_df.to_dict('records')[m
[31m-[m
[31m-            for resource_dict in resource_dict_list:[m
[31m-                res_meta = resource_dict[m
[31m-                resource = portal.action.resource_create(**res_meta)[m
[31m-                log.info('resource_create: '+datetime.datetime.now().isoformat()+' -- '+str(resource)+'\n')[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-    [m
[31m-    def _finished_process(self, data_dict):[m
[31m-        portal = LocalCKAN()[m
[31m-        log_str = 'import finished: '+datetime.datetime.now().isoformat()+' -- ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\n'[m
[31m-        activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Finished", "import_log": log_str}, [m
[31m-            "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-            "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-            "activity_type": "changed user"[m
[31m-            }[m
[31m-        portal.action.activity_create(**activity_dict)[m
[31m-        log.info(log_str)[m
[31m-[m
[31m-    def import_dataset(self):[m
[31m-[m
[31m-        context = {'model': model, 'user': c.user, 'auth_user_obj': c.userobj}[m
[31m-        try:[m
[31m-            check_access('config_option_update', context, {})[m
[31m-        except logic.NotAuthorized:[m
[31m-            abort(403, _('Need to be system administrator to administer'))[m
[31m-[m
[31m-        items = [[m
[31m-            {'name': 'template_file', 'control': 'image_upload', 'label': _('Template File'), 'placeholder': '', 'upload_enabled':h.uploads_enabled(),[m
[31m-                'field_url': 'template_file', 'field_upload': 'template_file_upload', 'field_clear': 'clear_template_file_upload'},[m
[31m-        ][m
[31m-        data = request.POST[m
[31m-        if 'save' in data:[m
[31m-            try:[m
[31m-                # really?[m
[31m-                data_dict = logic.clean_dict([m
[31m-                    dict_fns.unflatten([m
[31m-                        logic.tuplize_dict([m
[31m-                            logic.parse_params([m
[31m-                                request.POST, ignore_keys=CACHE_PARAMETERS))))[m
[31m-[m
[31m-                del data_dict['save'][m
[31m-[m
[31m-                schema = schema_.update_configuration_schema()[m
[31m-[m
[31m-                upload = uploader.get_uploader('admin')[m
[31m-                upload.update_data_dict(data_dict, 'template_file',[m
[31m-                                    'template_file_upload', 'clear_template_file_upload')[m
[31m-                upload.upload(uploader.get_max_image_size())[m
[31m-[m
[31m-                data, errors = _validate(data_dict, schema, context)[m
[31m-                if errors:[m
[31m-                    model.Session.rollback()[m
[31m-                    raise ValidationError(errors)[m
[31m-[m
[31m-                for key, value in six.iteritems(data):[m
[31m-                [m
[31m-                    if key == 'template_file' and value and not value.startswith('http')\[m
[31m-                            and not value.startswith('/'):[m
[31m-                        image_path = 'uploads/admin/'[m
[31m-[m
[31m-                        value = h.url_for_static('{0}{1}'.format(image_path, value))[m
[31m-[m
[31m-                    # Update CKAN's `config` object[m
[31m-                    config[key] = value[m
[31m-[m
[31m-                log.info('Import Dataset: {0}'.format(data))[m
[31m-                [m
[31m-                import_uuid = str(uuid.uuid4())[m
[31m-                filename = str(config['ckan.storage_path'])+'/storage/uploads/admin/'+data['template_file'][m
[31m-                template_org = data['template_org'] or 'all'[m
[31m-                owner_org = data['import_org'][m
[31m-                importer = c.user[m
[31m-                data_dict = {"import_uuid":import_uuid, "template_org":template_org, "owner_org":owner_org, "filename":filename, "importer":importer}[m
[31m-                log.info('Prepare to import data import_id:%r file:%r org:%r to_org:%r user:%r',import_uuid, filename, template_org, owner_org, importer)[m
[31m-[m
[31m-                row_count = 0[m
[31m-[m
[31m-                record_df = pd.read_excel(filename, header=[3], sheet_name='Temp2_Meta_Record')[m
[31m-                if template_org != 'all':[m
[31m-                    row_count += record_df.iloc[:, 3].tolist().count(template_org)[m
[31m-                else:[m
[31m-                    row_count += (len(record_df.index)-1)[m
[31m-                [m
[31m-                stat_df = pd.read_excel(filename, header=[3], sheet_name='Temp2_Meta_Stat')[m
[31m-                if template_org != 'all':[m
[31m-                    row_count += stat_df.iloc[:, 3].tolist().count(template_org)[m
[31m-                else:[m
[31m-                    row_count += (len(stat_df.index)-1)[m
[31m-[m
[31m-                gis_df = pd.read_excel(filename, header=[3], sheet_name='Temp2_Meta_GIS')[m
[31m-                if template_org != 'all':[m
[31m-                    row_count += gis_df.iloc[:, 3].tolist().count(template_org)[m
[31m-                else:[m
[31m-                    row_count += (len(gis_df.index)-1)[m
[31m-[m
[31m-                multi_df = pd.read_excel(filename, header=[3], sheet_name='Temp2_Meta_Multi')[m
[31m-                if template_org != 'all':[m
[31m-                    row_count += multi_df.iloc[:, 3].tolist().count(template_org)[m
[31m-                else:[m
[31m-                    row_count += (len(multi_df.index)-1)[m
[31m-[m
[31m-                other_df = pd.read_excel(filename, header=[3], sheet_name='Temp2_Meta_Other')[m
[31m-                if template_org != 'all':[m
[31m-                    row_count += other_df.iloc[:, 3].tolist().count(template_org)[m
[31m-                else:[m
[31m-                    row_count += (len(other_df.index)-1)[m
[31m- [m
[31m-                toolkit.get_action('dataset_bulk_import')(context, data_dict)[m
[31m-[m
[31m-                data_dict['row'] = row_count[m
[31m-                config["import_log"] = ''[m
[31m-                config['ckan.import_params'] = data_dict[m
[31m-                config['ckan.import_uuid'] = import_uuid[m
[31m-                config['ckan.import_row'] = row_count[m
[31m-[m
[31m-                model.set_system_info('ckan.import_params', data_dict)[m
[31m-                model.set_system_info('ckan.import_uuid', import_uuid)[m
[31m-                model.set_system_info('ckan.import_row', row_count)[m
[31m-            except logic.ValidationError as e:[m
[31m-                errors = e.error_dict[m
[31m-                error_summary = e.error_summary[m
[31m-                vars = {'data': data, 'errors': errors,[m
[31m-                        'error_summary': error_summary, 'form_items': items}[m
[31m-                return render('admin/dataset_import_form.html', extra_vars=vars)[m
[31m-[m
[31m-            h.redirect_to(controller='ckanext.thai_gdc.controllers.dataset:DatasetImportController', action='import_dataset')[m
[31m-[m
[31m-        schema = logic.schema.update_configuration_schema()[m
[31m-        data = {}[m
[31m-        for key in schema:[m
[31m-            data[key] = config.get(key)[m
[31m-[m
[31m-        vars = {'data': data, 'errors': {}, 'form_items': items}[m
[31m-        return render('admin/dataset_import_form.html', extra_vars=vars)[m
[31m-    [m
[31m-    def clear_import_log(self):[m
[31m-        [m
[31m-        config["import_log"] = ''[m
[31m-        config['template_file'] = ''[m
[31m-        config['import_org'] = ''[m
[31m-        config['template_org'] = ''[m
[31m-        config['ckan.import_params'] = ''[m
[31m-        config['ckan.import_uuid'] = ''[m
[31m-        config['ckan.import_row'] = ''[m
[31m-[m
[31m-        return render('admin/clear_import_log.html')[m
[31m-[m
[1mdiff --git a/ckanext/thai_gdc/controllers/dataset.py.bak b/ckanext/thai_gdc/controllers/dataset.py.bak[m
[1mdeleted file mode 100644[m
[1mindex cf4f90b..0000000[m
[1m--- a/ckanext/thai_gdc/controllers/dataset.py.bak[m
[1m+++ /dev/null[m
[36m@@ -1,942 +0,0 @@[m
[31m-# -*- coding: utf-8 -*-[m
[31m-import ckan.plugins as p[m
[31m-from pylons import config[m
[31m-import ckan.logic as logic[m
[31m-import ckan.lib.navl.dictization_functions as dict_fns[m
[31m-import ckan.model as model[m
[31m-import ckan.lib.uploader as uploader[m
[31m-import six[m
[31m-import logging[m
[31m-import pandas as pd[m
[31m-import numpy as np[m
[31m-from ckanapi import LocalCKAN[m
[31m-import datetime[m
[31m-[m
[31m-import uuid[m
[31m-import ckan.plugins.toolkit as toolkit[m
[31m-[m
[31m-from ckan.plugins.toolkit import ([m
[31m-    _, c, h, check_access, abort, render, request[m
[31m-    )[m
[31m-[m
[31m-from ckan.controllers.home import CACHE_PARAMETERS[m
[31m-import ckan.logic.schema as schema_[m
[31m-[m
[31m-_validate = dict_fns.validate[m
[31m-ValidationError = logic.ValidationError[m
[31m-NotFound = logic.NotFound[m
[31m-NotAuthorized = logic.NotAuthorized[m
[31m-[m
[31m-log = logging.getLogger(__name__)[m
[31m-[m
[31m-class DatasetManageController(p.toolkit.BaseController):[m
[31m-[m
[31m-    def datatype_patch(self, package_id):[m
[31m-        data = request.GET[m
[31m-        if 'data_type' in data:[m
[31m-            try:[m
[31m-                data_dict = logic.clean_dict([m
[31m-                    dict_fns.unflatten([m
[31m-                        logic.tuplize_dict([m
[31m-                            logic.parse_params([m
[31m-                                request.GET, ignore_keys=CACHE_PARAMETERS))))[m
[31m-                portal = LocalCKAN()[m
[31m-                patch_meta = {'id':package_id,'data_type':data['data_type']}[m
[31m-                package = portal.action.package_patch(**patch_meta)[m
[31m-                h.redirect_to(controller='dataset', action='read', id=package_id)[m
[31m-            except logic.ValidationError as e:[m
[31m-                return e[m
[31m-[m
[31m-class DatasetImportController(p.toolkit.BaseController):[m
[31m-[m
[31m-    def _record_type_process(self, data_dict):[m
[31m-        try:[m
[31m-            record_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp2_Meta_Record')[m
[31m-            record_df.drop(0, inplace=True)[m
[31m-            record_df["data_type"] = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô'[m
[31m-[m
[31m-            record_df.columns = ['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'][m
[31m-            record_df.drop(['d_type'], axis=1, inplace=True)[m
[31m-            record_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            record_df = record_df.astype('unicode')[m
[31m-            record_df = record_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-            record_df['high_value_dataset'] = np.where(record_df['high_value_dataset'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            record_df['reference_data'] = np.where(record_df['reference_data'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-[m
[31m-            record_df["dataset_name"] = record_df["name"][m
[31m-            record_df["name"] = record_df["name"].str.lower()[m
[31m-            record_df["name"].replace('\s', '-', regex=True, inplace=True)[m
[31m-            if data_dict['template_org'] != 'all':[m
[31m-                record_df = record_df.loc[record_df['owner_org'] == data_dict['template_org']][m
[31m-                record_df.reset_index(drop=True, inplace=True)[m
[31m-            record_df["owner_org"] = data_dict['owner_org'][m
[31m-            record_df["private"] = True[m
[31m-            record_df["allow_harvest"] = "False"[m
[31m-            record_df['tag_string'] = record_df['tag_string'].str.split(',').apply(lambda x: [e.strip() for e in x]).tolist()[m
[31m-[m
[31m-            record_df["created_date"] = pd.to_datetime((pd.to_numeric(record_df["created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+record_df["created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            record_df["last_updated_date"] = pd.to_datetime((pd.to_numeric(record_df["last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+record_df["last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-[m
[31m-            objective_choices = [u'‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏£‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡πÅ‡∏ú‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 3 (‡∏°‡∏ï‡∏¥‡∏Ñ‡∏£‡∏°. 4 ‡∏ò.‡∏Ñ. 2560)',u'‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•/‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô',u'‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á',u'‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô',u'‡∏î‡∏±‡∏ä‡∏ô‡∏µ/‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            record_df['objective_other'] = record_df['objective'].isin(objective_choices)[m
[31m-            record_df['objective_other'] = np.where(record_df['objective_other'], 'True', record_df['objective'])[m
[31m-            record_df['objective'] = np.where(record_df['objective_other'] == 'True', record_df['objective'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['objective_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            update_frequency_unit_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', u'‡∏õ‡∏µ', u'‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏õ‡∏µ',u'‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™',u'‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',u'‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå',u'‡∏ß‡∏±‡∏ô',u'‡∏ß‡∏±‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£',u'‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á',u'‡∏ô‡∏≤‡∏ó‡∏µ',u'‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á',u'‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'][m
[31m-            record_df['update_frequency_unit_other'] = record_df['update_frequency_unit'].isin(update_frequency_unit_choices)[m
[31m-            record_df['update_frequency_unit_other'] = np.where(record_df['update_frequency_unit_other'], 'True', record_df['update_frequency_unit'])[m
[31m-            record_df['update_frequency_unit'] = np.where(record_df['update_frequency_unit_other'] == 'True', record_df['update_frequency_unit'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['update_frequency_unit_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            geo_coverage_choices = [u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÇ‡∏•‡∏Å', u'‡∏ó‡∏ß‡∏µ‡∏õ/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏ó‡∏ß‡∏µ‡∏õ',u'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',u'‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏†‡∏≤‡∏Ñ',u'‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',u'‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',u'‡∏ï‡∏≥‡∏ö‡∏•',u'‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô',u'‡πÄ‡∏ó‡∏®‡∏ö‡∏≤‡∏•/‡∏≠‡∏ö‡∏ï.',u'‡∏û‡∏¥‡∏Å‡∏±‡∏î',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            record_df['geo_coverage_other'] = record_df['geo_coverage'].isin(geo_coverage_choices)[m
[31m-            record_df['geo_coverage_other'] = np.where(record_df['geo_coverage_other'], 'True', record_df['geo_coverage'])[m
[31m-            record_df['geo_coverage'] = np.where(record_df['geo_coverage_other'] == 'True', record_df['geo_coverage'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['geo_coverage_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_format_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', 'Database', 'CSV','XML','Image','Video','Audio','Text','JSON','HTML','DOC/DOCX','XLS','PDF','RDF','NoSQL','Arc/Info Coverage','Shapefile','GeoTiff','GML'][m
[31m-            record_df['data_format_other'] = record_df['data_format'].isin(data_format_choices)[m
[31m-            record_df['data_format_other'] = np.where(record_df['data_format_other'], 'True', record_df['data_format'])[m
[31m-            record_df['data_format'] = np.where(record_df['data_format_other'] == 'True', record_df['data_format'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['data_format_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            license_id_choices = ['Open Data Common', 'Creative Commons Attributions','Creative Commons Attribution Non-Commercial','Creative Commons Attribution Share-Alike','Creative Commons Attribution Non-Commercial Share-Alike','Creative Commons Attribution No-Derivs','Creative Commons Attribution Non-Commercial No-Derivs'][m
[31m-            record_df['license_id_other'] = record_df['license_id'].isin(license_id_choices)[m
[31m-            record_df['license_id_other'] = np.where(record_df['license_id_other'], 'True', record_df['license_id'])[m
[31m-            record_df['license_id'] = np.where(record_df['license_id_other'] == 'True', record_df['license_id'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['license_id_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            data_support_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏ä‡∏ô',u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô/‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ò‡∏¥/‡∏™‡∏°‡∏≤‡∏Ñ‡∏°',u'‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤'][m
[31m-            record_df['data_support_other'] = record_df['data_support'].isin(data_support_choices)[m
[31m-            record_df['data_support_other'] = np.where(record_df['data_support_other'], 'True', record_df['data_support'])[m
[31m-            record_df['data_support'] = np.where(record_df['data_support_other'] == 'True', record_df['data_support'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['data_support_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_collect_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ',u'‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•', u'‡∏Ñ‡∏£‡∏±‡∏ß‡πÄ‡∏£‡∏∑‡∏≠‡∏ô/‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß', u'‡∏ö‡πâ‡∏≤‡∏ô/‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏≤‡∏®‡∏±‡∏¢',u'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏´‡πâ‡∏≤‡∏á‡∏£‡πâ‡∏≤‡∏ô/‡∏™‡∏ñ‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£',u'‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£/‡∏™‡∏¥‡πà‡∏á‡∏õ‡∏•‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á',u'‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏©‡∏ï‡∏£ ‡∏õ‡∏£‡∏∞‡∏°‡∏á ‡∏õ‡πà‡∏≤‡πÑ‡∏°‡πâ',u'‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏û‡∏∑‡∏ä',u'‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà',u'‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ô‡πâ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏°‡πà‡∏ô‡πâ‡∏≥ ‡∏≠‡πà‡∏≤‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏ô‡πâ‡∏≥',u'‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡∏ô‡∏ô ‡∏ó‡∏≤‡∏á‡∏£‡∏ñ‡πÑ‡∏ü',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            record_df['data_collect_other'] = record_df['data_collect'].isin(data_collect_choices)[m
[31m-            record_df['data_collect_other'] = np.where(record_df['data_collect_other'], 'True', record_df['data_collect'])[m
[31m-            record_df['data_collect'] = np.where(record_df['data_collect_other'] == 'True', record_df['data_collect'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['data_collect_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_language_choices = ['',u'‡πÑ‡∏ó‡∏¢', u'‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©', u'‡∏à‡∏µ‡∏ô',u'‡∏°‡∏•‡∏≤‡∏¢‡∏π',u'‡∏û‡∏°‡πà‡∏≤',u'‡∏•‡∏≤‡∏ß',u'‡πÄ‡∏Ç‡∏°‡∏£',u'‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô',u'‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ',u'‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™',u'‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏±‡∏ô',u'‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            record_df['data_language_other'] = record_df['data_language'].isin(data_language_choices)[m
[31m-            record_df['data_language_other'] = np.where(record_df['data_language_other'], 'True', record_df['data_language'])[m
[31m-            record_df['data_language'] = np.where(record_df['data_language_other'] == 'True', record_df['data_language'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            record_df['data_language_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            record_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            [m
[31m-        except Exception as err:[m
[31m-           log.info(err)[m
[31m-           record_df = pd.DataFrame(columns=['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'])[m
[31m-           record_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-           record_df = record_df.astype('unicode')[m
[31m-           record_df = record_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-            [m
[31m-        portal = LocalCKAN()[m
[31m-[m
[31m-        package_dict_list = record_df.to_dict('records')[m
[31m-        for pkg_meta in package_dict_list:[m
[31m-            try:[m
[31m-                if pkg_meta['data_language'] == '':[m
[31m-                    pkg_meta.pop('data_language', None)[m
[31m-                    pkg_meta.pop('data_language_other', None)[m
[31m-                package = portal.action.package_create(**pkg_meta)[m
[31m-                log_str = 'package_create: '+datetime.datetime.now().isoformat()+' -- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(package.get("name"))+' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n'[m
[31m-                activity_dict = {"data": {"actor": six.ensure_text(data_dict["importer"]), "package":package, [m
[31m-                    "import": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": package.get("id"), [m
[31m-                    "activity_type": "new package"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-                record_df.loc[record_df['name'] == pkg_meta['name'], 'success'] = '1'[m
[31m-            except Exception as err:[m
[31m-                record_df.loc[record_df['name'] == pkg_meta['name'], 'success'] = '0'[m
[31m-                log_str = 'package_error: '+datetime.datetime.now().isoformat()+' -- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(pkg_meta['name'])+' : '+str(err)+'\n'[m
[31m-                activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "activity_type": "changed user"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-[m
[31m-        try:[m
[31m-            resource_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp3_Resource_Record')[m
[31m-            resource_df.drop(0, inplace=True)[m
[31m-            resource_df.columns = ['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-        except:[m
[31m-            resource_df = pd.DataFrame(columns=['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'])[m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        try:[m
[31m-            final_df = pd.merge(record_df,resource_df,how='left',left_on='dataset_name',right_on='dataset_name')[m
[31m-            final_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = final_df[(final_df['resource_url'] != '') & (final_df['success'] == '1')][m
[31m-            resource_df = resource_df[['name','success','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect']][m
[31m-            resource_df.columns = ['package_id','success','name','url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df["resource_created_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_last_updated_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df['created'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df['last_modified'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            resource_dict_list = resource_df.to_dict('records')[m
[31m-[m
[31m-            for resource_dict in resource_dict_list:[m
[31m-                res_meta = resource_dict[m
[31m-                resource = portal.action.resource_create(**res_meta)[m
[31m-                log.info('resource_create: '+datetime.datetime.now().isoformat()+' -- '+str(resource)+'\n')[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-[m
[31m-    def _stat_type_process(self, data_dict):[m
[31m-        try:[m
[31m-            stat_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp2_Meta_Stat')[m
[31m-            stat_df.drop(0, inplace=True)[m
[31m-            stat_df["data_type"] = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥'[m
[31m-[m
[31m-            stat_df.columns = ['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','first_year_of_data','last_year_of_data','data_release_calendar','last_updated_date','disaggregate','unit_of_measure','unit_of_multiplier','calculation_method','standard','url','data_language','official_statistics','data_type'][m
[31m-            stat_df.drop(['d_type'], axis=1, inplace=True)[m
[31m-            stat_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            stat_df = stat_df.astype('unicode')[m
[31m-            stat_df = stat_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-            stat_df['official_statistics'] = np.where(stat_df['official_statistics'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            [m
[31m-            stat_df["dataset_name"] = stat_df["name"][m
[31m-            stat_df["name"] = stat_df["name"].str.lower()[m
[31m-            stat_df["name"].replace('\s', '-', regex=True, inplace=True)[m
[31m-            if data_dict['template_org'] != 'all':[m
[31m-                stat_df = stat_df.loc[stat_df['owner_org'] == data_dict['template_org']][m
[31m-                stat_df.reset_index(drop=True, inplace=True)[m
[31m-            stat_df["owner_org"] = data_dict['owner_org'][m
[31m-            stat_df["private"] = True[m
[31m-            stat_df["allow_harvest"] = "False"[m
[31m-            stat_df['tag_string'] = stat_df['tag_string'].str.split(',').apply(lambda x: [e.strip() for e in x]).tolist()[m
[31m-[m
[31m-            stat_df["last_updated_date"] = pd.to_datetime((pd.to_numeric(stat_df["last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+stat_df["last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-[m
[31m-            objective_choices = [u'‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏£‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡πÅ‡∏ú‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 3 (‡∏°‡∏ï‡∏¥‡∏Ñ‡∏£‡∏°. 4 ‡∏ò.‡∏Ñ. 2560)',u'‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•/‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô',u'‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á',u'‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô',u'‡∏î‡∏±‡∏ä‡∏ô‡∏µ/‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            stat_df['objective_other'] = stat_df['objective'].isin(objective_choices)[m
[31m-            stat_df['objective_other'] = np.where(stat_df['objective_other'], 'True', stat_df['objective'])[m
[31m-            stat_df['objective'] = np.where(stat_df['objective_other'] == 'True', stat_df['objective'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['objective_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            update_frequency_unit_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', u'‡∏õ‡∏µ', u'‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏õ‡∏µ',u'‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™',u'‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',u'‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå',u'‡∏ß‡∏±‡∏ô',u'‡∏ß‡∏±‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£',u'‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á',u'‡∏ô‡∏≤‡∏ó‡∏µ',u'‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á',u'‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'][m
[31m-            stat_df['update_frequency_unit_other'] = stat_df['update_frequency_unit'].isin(update_frequency_unit_choices)[m
[31m-            stat_df['update_frequency_unit_other'] = np.where(stat_df['update_frequency_unit_other'], 'True', stat_df['update_frequency_unit'])[m
[31m-            stat_df['update_frequency_unit'] = np.where(stat_df['update_frequency_unit_other'] == 'True', stat_df['update_frequency_unit'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['update_frequency_unit_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            geo_coverage_choices = [u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÇ‡∏•‡∏Å', u'‡∏ó‡∏ß‡∏µ‡∏õ/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏ó‡∏ß‡∏µ‡∏õ',u'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',u'‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏†‡∏≤‡∏Ñ',u'‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',u'‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',u'‡∏ï‡∏≥‡∏ö‡∏•',u'‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô',u'‡πÄ‡∏ó‡∏®‡∏ö‡∏≤‡∏•/‡∏≠‡∏ö‡∏ï.',u'‡∏û‡∏¥‡∏Å‡∏±‡∏î',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            stat_df['geo_coverage_other'] = stat_df['geo_coverage'].isin(geo_coverage_choices)[m
[31m-            stat_df['geo_coverage_other'] = np.where(stat_df['geo_coverage_other'], 'True', stat_df['geo_coverage'])[m
[31m-            stat_df['geo_coverage'] = np.where(stat_df['geo_coverage_other'] == 'True', stat_df['geo_coverage'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['geo_coverage_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_format_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', 'Database', 'CSV','XML','Image','Video','Audio','Text','JSON','HTML','DOC/DOCX','XLS','PDF','RDF','NoSQL','Arc/Info Coverage','Shapefile','GeoTiff','GML'][m
[31m-            stat_df['data_format_other'] = stat_df['data_format'].isin(data_format_choices)[m
[31m-            stat_df['data_format_other'] = np.where(stat_df['data_format_other'], 'True', stat_df['data_format'])[m
[31m-            stat_df['data_format'] = np.where(stat_df['data_format_other'] == 'True', stat_df['data_format'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['data_format_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            license_id_choices = ['Open Data Common', 'Creative Commons Attributions','Creative Commons Attribution Non-Commercial','Creative Commons Attribution Share-Alike','Creative Commons Attribution Non-Commercial Share-Alike','Creative Commons Attribution No-Derivs','Creative Commons Attribution Non-Commercial No-Derivs'][m
[31m-            stat_df['license_id_other'] = stat_df['license_id'].isin(license_id_choices)[m
[31m-            stat_df['license_id_other'] = np.where(stat_df['license_id_other'], 'True', stat_df['license_id'])[m
[31m-            stat_df['license_id'] = np.where(stat_df['license_id_other'] == 'True', stat_df['license_id'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['license_id_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            stat_df["data_release_calendar"] = pd.to_datetime((pd.to_numeric(stat_df["data_release_calendar"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+stat_df["data_release_calendar"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            [m
[31m-            disaggregate_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÄ‡∏û‡∏®', u'‡∏≠‡∏≤‡∏¢‡∏∏/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏≤‡∏¢‡∏∏',u'‡∏™‡∏ñ‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡∏™‡∏°‡∏£‡∏™',u'‡∏®‡∏≤‡∏™‡∏ô‡∏≤',u'‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤',u'‡∏≠‡∏≤‡∏ä‡∏µ‡∏û',u'‡∏™‡∏ñ‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô',u'‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°/‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£',u'‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ',u'‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà',u'‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            stat_df['disaggregate_other'] = stat_df['disaggregate'].isin(disaggregate_choices)[m
[31m-            stat_df['disaggregate_other'] = np.where(stat_df['disaggregate_other'], 'True', stat_df['disaggregate'])[m
[31m-            stat_df['disaggregate'] = np.where(stat_df['disaggregate_other'] == 'True', stat_df['disaggregate'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['disaggregate_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            unit_of_multiplier_choices = ['',u'‡∏´‡∏ô‡πà‡∏ß‡∏¢', u'‡∏™‡∏¥‡∏ö', u'‡∏£‡πâ‡∏≠‡∏¢',u'‡∏û‡∏±‡∏ô',u'‡∏´‡∏°‡∏∑‡πà‡∏ô',u'‡πÅ‡∏™‡∏ô',u'‡∏•‡πâ‡∏≤‡∏ô',u'‡∏™‡∏¥‡∏ö‡∏•‡πâ‡∏≤‡∏ô',u'‡∏£‡πâ‡∏≠‡∏¢‡∏•‡πâ‡∏≤‡∏ô',u'‡∏û‡∏±‡∏ô‡∏•‡πâ‡∏≤‡∏ô',u'‡∏´‡∏°‡∏∑‡πà‡∏ô‡∏•‡πâ‡∏≤‡∏ô',u'‡πÅ‡∏™‡∏ô‡∏•‡πâ‡∏≤‡∏ô',u'‡∏•‡πâ‡∏≤‡∏ô‡∏•‡πâ‡∏≤‡∏ô',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            stat_df['unit_of_multiplier_other'] = stat_df['unit_of_multiplier'].isin(unit_of_multiplier_choices)[m
[31m-            stat_df['unit_of_multiplier_other'] = np.where(stat_df['unit_of_multiplier_other'], 'True', stat_df['unit_of_multiplier'])[m
[31m-            stat_df['unit_of_multiplier'] = np.where(stat_df['unit_of_multiplier_other'] == 'True', stat_df['unit_of_multiplier'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['unit_of_multiplier_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            data_language_choices = ['',u'‡πÑ‡∏ó‡∏¢', u'‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©', u'‡∏à‡∏µ‡∏ô',u'‡∏°‡∏•‡∏≤‡∏¢‡∏π',u'‡∏û‡∏°‡πà‡∏≤',u'‡∏•‡∏≤‡∏ß',u'‡πÄ‡∏Ç‡∏°‡∏£',u'‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô',u'‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ',u'‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™',u'‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏±‡∏ô',u'‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            stat_df['data_language_other'] = stat_df['data_language'].isin(data_language_choices)[m
[31m-            stat_df['data_language_other'] = np.where(stat_df['data_language_other'], 'True', stat_df['data_language'])[m
[31m-            stat_df['data_language'] = np.where(stat_df['data_language_other'] == 'True', stat_df['data_language'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            stat_df['data_language_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            stat_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            [m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-            stat_df = pd.DataFrame(columns=['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','first_year_of_data','last_year_of_data','data_release_calendar','last_updated_date','disaggregate','unit_of_measure','unit_of_multiplier','calculation_method','standard','url','data_language','official_statistics','data_type'])[m
[31m-            stat_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            stat_df = stat_df.astype('unicode')[m
[31m-            stat_df = stat_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        portal = LocalCKAN()[m
[31m-[m
[31m-        package_dict_list = stat_df.to_dict('records')[m
[31m-        for pkg_meta in package_dict_list:[m
[31m-            try:[m
[31m-                if pkg_meta['disaggregate'] == '':[m
[31m-                    pkg_meta.pop('disaggregate', None)[m
[31m-                    pkg_meta.pop('disaggregate_other', None)[m
[31m-                if pkg_meta['data_language'] == '':[m
[31m-                    pkg_meta.pop('data_language', None)[m
[31m-                    pkg_meta.pop('data_language_other', None)[m
[31m-                package = portal.action.package_create(**pkg_meta)[m
[31m-                log_str = 'package_create: '+datetime.datetime.now().isoformat()+' -- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(package.get("name"))+' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n'[m
[31m-                activity_dict = {"data": {"actor": six.ensure_text(data_dict["importer"]), "package":package, [m
[31m-                    "import": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": package.get("id"), [m
[31m-                    "activity_type": "new package"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-                stat_df.loc[stat_df['name'] == pkg_meta['name'], 'success'] = '1'[m
[31m-            except Exception as err:[m
[31m-                stat_df.loc[stat_df['name'] == pkg_meta['name'], 'success'] = '0'[m
[31m-                log_str = 'package_error: '+datetime.datetime.now().isoformat()+' -- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(pkg_meta['name'])+' : '+str(err)+'\n'[m
[31m-                activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "activity_type": "changed user"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-[m
[31m-        try:[m
[31m-            resource_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp3_Resource_Stat')[m
[31m-            resource_df.drop(0, inplace=True)[m
[31m-            resource_df.columns = ['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_first_year_of_data','resource_last_year_of_data','resource_data_release_calendar','resource_disaggregate','resource_unit_of_measure','resource_unit_of_multiplier','resource_official_statistics'][m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-        except:[m
[31m-            resource_df = pd.DataFrame(columns=['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_first_year_of_data','resource_last_year_of_data','resource_data_release_calendar','resource_disaggregate','resource_unit_of_measure','resource_unit_of_multiplier','resource_official_statistics'])[m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        try:[m
[31m-            final_df = pd.merge(stat_df,resource_df,how='left',left_on='dataset_name',right_on='dataset_name')[m
[31m-            final_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = final_df[(final_df['resource_url'] != '') & (final_df['success'] == '1')][m
[31m-            resource_df = resource_df[['name','success','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_first_year_of_data','resource_last_year_of_data','resource_data_release_calendar','resource_disaggregate','resource_unit_of_measure','resource_unit_of_multiplier','resource_official_statistics']][m
[31m-            resource_df.columns = ['package_id','success','name','url','description','resource_accessible_condition','resource_last_updated_date','format','resource_first_year_of_data','resource_last_year_of_data','resource_data_release_calendar','resource_disaggregate','resource_unit_of_measure','resource_unit_of_multiplier','resource_official_statistics'][m
[31m-[m
[31m-            disaggregate_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÄ‡∏û‡∏®', u'‡∏≠‡∏≤‡∏¢‡∏∏/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏≠‡∏≤‡∏¢‡∏∏',u'‡∏™‡∏ñ‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡∏™‡∏°‡∏£‡∏™',u'‡∏®‡∏≤‡∏™‡∏ô‡∏≤',u'‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤',u'‡∏≠‡∏≤‡∏ä‡∏µ‡∏û',u'‡∏™‡∏ñ‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô',u'‡∏≠‡∏∏‡∏ï‡∏™‡∏≤‡∏´‡∏Å‡∏£‡∏£‡∏°/‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£',u'‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ',u'‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà',u'‡∏ú‡∏•‡∏¥‡∏ï‡∏†‡∏±‡∏ì‡∏ë‡πå',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            resource_df['resource_disaggregate_other'] = resource_df['resource_disaggregate'].isin(disaggregate_choices)[m
[31m-            resource_df['resource_disaggregate_other'] = np.where(resource_df['resource_disaggregate_other'], 'True', resource_df['resource_disaggregate'])[m
[31m-            resource_df['resource_disaggregate'] = np.where(resource_df['resource_disaggregate_other'] == 'True', resource_df['resource_disaggregate'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            resource_df['resource_disaggregate_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            resource_df["resource_data_release_calendar"] = pd.to_datetime((pd.to_numeric(resource_df["resource_data_release_calendar"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_data_release_calendar"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_last_updated_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df['created'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df['last_modified'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            resource_dict_list = resource_df.to_dict('records')[m
[31m-[m
[31m-            for resource_dict in resource_dict_list:[m
[31m-                res_meta = resource_dict[m
[31m-                if res_meta['resource_disaggregate'] == '':[m
[31m-                    res_meta.pop('resource_disaggregate', None)[m
[31m-                    res_meta.pop('resource_disaggregate_other', None)[m
[31m-                resource = portal.action.resource_create(**res_meta)[m
[31m-                log.info('resource_create: '+datetime.datetime.now().isoformat()+' -- '+str(resource)+'\n')[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-[m
[31m-    def _gis_type_process(self, data_dict):[m
[31m-        try:[m
[31m-            gis_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp2_Meta_GIS')[m
[31m-            gis_df.drop(0, inplace=True)[m
[31m-            gis_df["data_type"] = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏π‡∏°‡∏¥‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà'[m
[31m-[m
[31m-            gis_df.columns = ['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','geographic_data_set','equivalent_scale','west_bound_longitude','east_bound_longitude','north_bound_longitude','south_bound_longitude','positional_accuracy','reference_period','last_updated_date','data_release_calendar','data_release_date','url','data_language','data_type'][m
[31m-            gis_df.drop(['d_type'], axis=1, inplace=True)[m
[31m-            gis_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            gis_df = gis_df.astype('unicode')[m
[31m-            gis_df = gis_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-            gis_df["dataset_name"] = gis_df["name"][m
[31m-            gis_df["name"] = gis_df["name"].str.lower()[m
[31m-            gis_df["name"].replace('\s', '-', regex=True, inplace=True)[m
[31m-            if data_dict['template_org'] != 'all':[m
[31m-                gis_df = gis_df.loc[gis_df['owner_org'] == data_dict['template_org']][m
[31m-                gis_df.reset_index(drop=True, inplace=True)[m
[31m-            gis_df["owner_org"] = data_dict['owner_org'][m
[31m-            gis_df["private"] = True[m
[31m-            gis_df["allow_harvest"] = "False"[m
[31m-            gis_df['tag_string'] = gis_df['tag_string'].str.split(',').apply(lambda x: [e.strip() for e in x]).tolist()[m
[31m-[m
[31m-            gis_df["last_updated_date"] = pd.to_datetime((pd.to_numeric(gis_df["last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+gis_df["last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-[m
[31m-            objective_choices = [u'‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏£‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡πÅ‡∏ú‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 3 (‡∏°‡∏ï‡∏¥‡∏Ñ‡∏£‡∏°. 4 ‡∏ò.‡∏Ñ. 2560)',u'‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•/‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô',u'‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á',u'‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô',u'‡∏î‡∏±‡∏ä‡∏ô‡∏µ/‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            gis_df['objective_other'] = gis_df['objective'].isin(objective_choices)[m
[31m-            gis_df['objective_other'] = np.where(gis_df['objective_other'], 'True', gis_df['objective'])[m
[31m-            gis_df['objective'] = np.where(gis_df['objective_other'] == 'True', gis_df['objective'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['objective_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            update_frequency_unit_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', u'‡∏õ‡∏µ', u'‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏õ‡∏µ',u'‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™',u'‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',u'‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå',u'‡∏ß‡∏±‡∏ô',u'‡∏ß‡∏±‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£',u'‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á',u'‡∏ô‡∏≤‡∏ó‡∏µ',u'‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á',u'‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'][m
[31m-            gis_df['update_frequency_unit_other'] = gis_df['update_frequency_unit'].isin(update_frequency_unit_choices)[m
[31m-            gis_df['update_frequency_unit_other'] = np.where(gis_df['update_frequency_unit_other'], 'True', gis_df['update_frequency_unit'])[m
[31m-            gis_df['update_frequency_unit'] = np.where(gis_df['update_frequency_unit_other'] == 'True', gis_df['update_frequency_unit'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['update_frequency_unit_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            geo_coverage_choices = [u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÇ‡∏•‡∏Å', u'‡∏ó‡∏ß‡∏µ‡∏õ/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏ó‡∏ß‡∏µ‡∏õ',u'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',u'‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏†‡∏≤‡∏Ñ',u'‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',u'‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',u'‡∏ï‡∏≥‡∏ö‡∏•',u'‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô',u'‡πÄ‡∏ó‡∏®‡∏ö‡∏≤‡∏•/‡∏≠‡∏ö‡∏ï.',u'‡∏û‡∏¥‡∏Å‡∏±‡∏î',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            gis_df['geo_coverage_other'] = gis_df['geo_coverage'].isin(geo_coverage_choices)[m
[31m-            gis_df['geo_coverage_other'] = np.where(gis_df['geo_coverage_other'], 'True', gis_df['geo_coverage'])[m
[31m-            gis_df['geo_coverage'] = np.where(gis_df['geo_coverage_other'] == 'True', gis_df['geo_coverage'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['geo_coverage_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_format_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', 'Database', 'CSV','XML','Image','Video','Audio','Text','JSON','HTML','DOC/DOCX','XLS','PDF','RDF','NoSQL','Arc/Info Coverage','Shapefile','GeoTiff','GML'][m
[31m-            gis_df['data_format_other'] = gis_df['data_format'].isin(data_format_choices)[m
[31m-            gis_df['data_format_other'] = np.where(gis_df['data_format_other'], 'True', gis_df['data_format'])[m
[31m-            gis_df['data_format'] = np.where(gis_df['data_format_other'] == 'True', gis_df['data_format'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['data_format_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            license_id_choices = ['Open Data Common', 'Creative Commons Attributions','Creative Commons Attribution Non-Commercial','Creative Commons Attribution Share-Alike','Creative Commons Attribution Non-Commercial Share-Alike','Creative Commons Attribution No-Derivs','Creative Commons Attribution Non-Commercial No-Derivs'][m
[31m-            gis_df['license_id_other'] = gis_df['license_id'].isin(license_id_choices)[m
[31m-            gis_df['license_id_other'] = np.where(gis_df['license_id_other'], 'True', gis_df['license_id'])[m
[31m-            gis_df['license_id'] = np.where(gis_df['license_id_other'] == 'True', gis_df['license_id'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['license_id_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            equivalent_scale_choices = ['','1:4,000', '1:10,000', '1:25,000','1:50,000','1:250,000'][m
[31m-            gis_df['equivalent_scale_other'] = gis_df['equivalent_scale'].isin(equivalent_scale_choices)[m
[31m-            gis_df['equivalent_scale_other'] = np.where(gis_df['equivalent_scale_other'], 'True', gis_df['equivalent_scale'])[m
[31m-            gis_df['equivalent_scale'] = np.where(gis_df['equivalent_scale_other'] == 'True', gis_df['equivalent_scale'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['equivalent_scale_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            gis_df["data_release_calendar"] = pd.to_datetime((pd.to_numeric(gis_df["data_release_calendar"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+gis_df["data_release_calendar"].str.slice(start=4),errors='coerce').astype(str)[m
[31m-            gis_df["data_release_date"] = pd.to_datetime((pd.to_numeric(gis_df["data_release_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+gis_df["data_release_date"].str.slice(start=4),errors='coerce').astype(str)[m
[31m-[m
[31m-            data_language_choices = ['',u'‡πÑ‡∏ó‡∏¢', u'‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©', u'‡∏à‡∏µ‡∏ô',u'‡∏°‡∏•‡∏≤‡∏¢‡∏π',u'‡∏û‡∏°‡πà‡∏≤',u'‡∏•‡∏≤‡∏ß',u'‡πÄ‡∏Ç‡∏°‡∏£',u'‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô',u'‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ',u'‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™',u'‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏±‡∏ô',u'‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            gis_df['data_language_other'] = gis_df['data_language'].isin(data_language_choices)[m
[31m-            gis_df['data_language_other'] = np.where(gis_df['data_language_other'], 'True', gis_df['data_language'])[m
[31m-            gis_df['data_language'] = np.where(gis_df['data_language_other'] == 'True', gis_df['data_language'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            gis_df['data_language_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            gis_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-            gis_df = pd.DataFrame(columns=['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','geographic_data_set','equivalent_scale','west_bound_longitude','east_bound_longitude','north_bound_longitude','south_bound_longitude','positional_accuracy','reference_period','last_updated_date','data_release_calendar','data_release_date','url','data_language','data_type'])[m
[31m-            gis_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            gis_df = gis_df.astype('unicode')[m
[31m-            gis_df = gis_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        portal = LocalCKAN()[m
[31m-[m
[31m-        package_dict_list = gis_df.to_dict('records')[m
[31m-        for pkg_meta in package_dict_list:[m
[31m-            try:[m
[31m-                if pkg_meta['data_language'] == '':[m
[31m-                    pkg_meta.pop('data_language', None)[m
[31m-                    pkg_meta.pop('data_language_other', None)[m
[31m-                package = portal.action.package_create(**pkg_meta)[m
[31m-                log_str = 'package_create: '+datetime.datetime.now().isoformat()+' -- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(package.get("name"))+' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n'[m
[31m-                activity_dict = {"data": {"actor": six.ensure_text(data_dict["importer"]), "package":package, [m
[31m-                    "import": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": package.get("id"), [m
[31m-                    "activity_type": "new package"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-                gis_df.loc[gis_df['name'] == pkg_meta['name'], 'success'] = '1'[m
[31m-            except Exception as err:[m
[31m-                gis_df.loc[gis_df['name'] == pkg_meta['name'], 'success'] = '0'[m
[31m-                log_str = 'package_error: '+datetime.datetime.now().isoformat()+' -- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(pkg_meta['name'])+' : '+str(err)+'\n'[m
[31m-                activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "activity_type": "changed user"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-[m
[31m-        try:[m
[31m-            resource_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp3_Resource_GIS')[m
[31m-            resource_df.drop(0, inplace=True)[m
[31m-            resource_df.columns = ['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_equivalent_scale','resource_geographic_data_set','resource_created_date','resource_data_release_date','resource_positional_accuracy'][m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-        except:[m
[31m-            resource_df = pd.DataFrame(columns=['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_equivalent_scale','resource_geographic_data_set','resource_created_date','resource_data_release_date','resource_positional_accuracy'])[m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        try:[m
[31m-            final_df = pd.merge(gis_df,resource_df,how='left',left_on='dataset_name',right_on='dataset_name')[m
[31m-            final_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = final_df[(final_df['resource_url'] != '') & (final_df['success'] == '1')][m
[31m-            resource_df = resource_df[['name','success','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_equivalent_scale','resource_geographic_data_set','resource_created_date','resource_data_release_date','resource_positional_accuracy']][m
[31m-            resource_df.columns = ['package_id','success','name','url','description','resource_accessible_condition','resource_last_updated_date','format','resource_equivalent_scale','resource_geographic_data_set','resource_created_date','resource_data_release_date','resource_positional_accuracy'][m
[31m-            resource_df["resource_created_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_last_updated_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_data_release_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_data_release_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_data_release_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df['created'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df['last_modified'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            resource_dict_list = resource_df.to_dict('records')[m
[31m-[m
[31m-            for resource_dict in resource_dict_list:[m
[31m-                res_meta = resource_dict[m
[31m-                resource = portal.action.resource_create(**res_meta)[m
[31m-                log.info('resource_create: '+datetime.datetime.now().isoformat()+' -- '+str(resource)+'\n')[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-[m
[31m-    def _multi_type_process(self, data_dict):[m
[31m-        try:[m
[31m-            multi_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp2_Meta_Multi')[m
[31m-            multi_df.drop(0, inplace=True)[m
[31m-            multi_df["data_type"] = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó'[m
[31m-[m
[31m-            multi_df.columns = ['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'][m
[31m-            multi_df.drop(['d_type'], axis=1, inplace=True)[m
[31m-            multi_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            multi_df = multi_df.astype('unicode')[m
[31m-            multi_df = multi_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-            multi_df['high_value_dataset'] = np.where(multi_df['high_value_dataset'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            multi_df['reference_data'] = np.where(multi_df['reference_data'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            [m
[31m-            multi_df["dataset_name"] = multi_df["name"][m
[31m-            multi_df["name"] = multi_df["name"].str.lower()[m
[31m-            multi_df["name"].replace('\s', '-', regex=True, inplace=True)[m
[31m-            if data_dict['template_org'] != 'all':[m
[31m-                multi_df = multi_df.loc[multi_df['owner_org'] == data_dict['template_org']][m
[31m-                multi_df.reset_index(drop=True, inplace=True)[m
[31m-            multi_df["owner_org"] = data_dict['owner_org'][m
[31m-            multi_df["private"] = True[m
[31m-            multi_df["allow_harvest"] = "False"[m
[31m-            multi_df['tag_string'] = multi_df['tag_string'].str.split(',').apply(lambda x: [e.strip() for e in x]).tolist()[m
[31m-[m
[31m-            multi_df["created_date"] = pd.to_datetime((pd.to_numeric(multi_df["created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+multi_df["created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            multi_df["last_updated_date"] = pd.to_datetime((pd.to_numeric(multi_df["last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+multi_df["last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-[m
[31m-            objective_choices = [u'‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏£‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡πÅ‡∏ú‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 3 (‡∏°‡∏ï‡∏¥‡∏Ñ‡∏£‡∏°. 4 ‡∏ò.‡∏Ñ. 2560)',u'‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•/‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô',u'‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á',u'‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô',u'‡∏î‡∏±‡∏ä‡∏ô‡∏µ/‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            multi_df['objective_other'] = multi_df['objective'].isin(objective_choices)[m
[31m-            multi_df['objective_other'] = np.where(multi_df['objective_other'], 'True', multi_df['objective'])[m
[31m-            multi_df['objective'] = np.where(multi_df['objective_other'] == 'True', multi_df['objective'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['objective_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            update_frequency_unit_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', u'‡∏õ‡∏µ', u'‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏õ‡∏µ',u'‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™',u'‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',u'‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå',u'‡∏ß‡∏±‡∏ô',u'‡∏ß‡∏±‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£',u'‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á',u'‡∏ô‡∏≤‡∏ó‡∏µ',u'‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á',u'‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'][m
[31m-            multi_df['update_frequency_unit_other'] = multi_df['update_frequency_unit'].isin(update_frequency_unit_choices)[m
[31m-            multi_df['update_frequency_unit_other'] = np.where(multi_df['update_frequency_unit_other'], 'True', multi_df['update_frequency_unit'])[m
[31m-            multi_df['update_frequency_unit'] = np.where(multi_df['update_frequency_unit_other'] == 'True', multi_df['update_frequency_unit'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['update_frequency_unit_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            geo_coverage_choices = [u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÇ‡∏•‡∏Å', u'‡∏ó‡∏ß‡∏µ‡∏õ/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏ó‡∏ß‡∏µ‡∏õ',u'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',u'‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏†‡∏≤‡∏Ñ',u'‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',u'‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',u'‡∏ï‡∏≥‡∏ö‡∏•',u'‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô',u'‡πÄ‡∏ó‡∏®‡∏ö‡∏≤‡∏•/‡∏≠‡∏ö‡∏ï.',u'‡∏û‡∏¥‡∏Å‡∏±‡∏î',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            multi_df['geo_coverage_other'] = multi_df['geo_coverage'].isin(geo_coverage_choices)[m
[31m-            multi_df['geo_coverage_other'] = np.where(multi_df['geo_coverage_other'], 'True', multi_df['geo_coverage'])[m
[31m-            multi_df['geo_coverage'] = np.where(multi_df['geo_coverage_other'] == 'True', multi_df['geo_coverage'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['geo_coverage_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_format_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', 'Database', 'CSV','XML','Image','Video','Audio','Text','JSON','HTML','DOC/DOCX','XLS','PDF','RDF','NoSQL','Arc/Info Coverage','Shapefile','GeoTiff','GML'][m
[31m-            multi_df['data_format_other'] = multi_df['data_format'].isin(data_format_choices)[m
[31m-            multi_df['data_format_other'] = np.where(multi_df['data_format_other'], 'True', multi_df['data_format'])[m
[31m-            multi_df['data_format'] = np.where(multi_df['data_format_other'] == 'True', multi_df['data_format'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['data_format_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            license_id_choices = ['Open Data Common', 'Creative Commons Attributions','Creative Commons Attribution Non-Commercial','Creative Commons Attribution Share-Alike','Creative Commons Attribution Non-Commercial Share-Alike','Creative Commons Attribution No-Derivs','Creative Commons Attribution Non-Commercial No-Derivs'][m
[31m-            multi_df['license_id_other'] = multi_df['license_id'].isin(license_id_choices)[m
[31m-            multi_df['license_id_other'] = np.where(multi_df['license_id_other'], 'True', multi_df['license_id'])[m
[31m-            multi_df['license_id'] = np.where(multi_df['license_id_other'] == 'True', multi_df['license_id'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['license_id_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            data_support_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏ä‡∏ô',u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô/‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ò‡∏¥/‡∏™‡∏°‡∏≤‡∏Ñ‡∏°',u'‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤'][m
[31m-            multi_df['data_support_other'] = multi_df['data_support'].isin(data_support_choices)[m
[31m-            multi_df['data_support_other'] = np.where(multi_df['data_support_other'], 'True', multi_df['data_support'])[m
[31m-            multi_df['data_support'] = np.where(multi_df['data_support_other'] == 'True', multi_df['data_support'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['data_support_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_collect_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ',u'‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•', u'‡∏Ñ‡∏£‡∏±‡∏ß‡πÄ‡∏£‡∏∑‡∏≠‡∏ô/‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß', u'‡∏ö‡πâ‡∏≤‡∏ô/‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏≤‡∏®‡∏±‡∏¢',u'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏´‡πâ‡∏≤‡∏á‡∏£‡πâ‡∏≤‡∏ô/‡∏™‡∏ñ‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£',u'‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£/‡∏™‡∏¥‡πà‡∏á‡∏õ‡∏•‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á',u'‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏©‡∏ï‡∏£ ‡∏õ‡∏£‡∏∞‡∏°‡∏á ‡∏õ‡πà‡∏≤‡πÑ‡∏°‡πâ',u'‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏û‡∏∑‡∏ä',u'‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà',u'‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ô‡πâ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏°‡πà‡∏ô‡πâ‡∏≥ ‡∏≠‡πà‡∏≤‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏ô‡πâ‡∏≥',u'‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡∏ô‡∏ô ‡∏ó‡∏≤‡∏á‡∏£‡∏ñ‡πÑ‡∏ü',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            multi_df['data_collect_other'] = multi_df['data_collect'].isin(data_collect_choices)[m
[31m-            multi_df['data_collect_other'] = np.where(multi_df['data_collect_other'], 'True', multi_df['data_collect'])[m
[31m-            multi_df['data_collect'] = np.where(multi_df['data_collect_other'] == 'True', multi_df['data_collect'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['data_collect_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_language_choices = ['',u'‡πÑ‡∏ó‡∏¢', u'‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©', u'‡∏à‡∏µ‡∏ô',u'‡∏°‡∏•‡∏≤‡∏¢‡∏π',u'‡∏û‡∏°‡πà‡∏≤',u'‡∏•‡∏≤‡∏ß',u'‡πÄ‡∏Ç‡∏°‡∏£',u'‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô',u'‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ',u'‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™',u'‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏±‡∏ô',u'‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            multi_df['data_language_other'] = multi_df['data_language'].isin(data_language_choices)[m
[31m-            multi_df['data_language_other'] = np.where(multi_df['data_language_other'], 'True', multi_df['data_language'])[m
[31m-            multi_df['data_language'] = np.where(multi_df['data_language_other'] == 'True', multi_df['data_language'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            multi_df['data_language_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            multi_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            [m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-            multi_df = pd.DataFrame(columns=['name','d_type','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'])[m
[31m-            multi_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            multi_df = multi_df.astype('unicode')[m
[31m-            multi_df = multi_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-            [m
[31m-        portal = LocalCKAN()[m
[31m-[m
[31m-        package_dict_list = multi_df.to_dict('records')[m
[31m-        for pkg_meta in package_dict_list:[m
[31m-            try:[m
[31m-                if pkg_meta['data_language'] == '':[m
[31m-                    pkg_meta.pop('data_language', None)[m
[31m-                    pkg_meta.pop('data_language_other', None)[m
[31m-                package = portal.action.package_create(**pkg_meta)[m
[31m-                log_str = 'package_create: '+datetime.datetime.now().isoformat()+' -- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(package.get("name"))+' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n'[m
[31m-                activity_dict = {"data": {"actor": six.ensure_text(data_dict["importer"]), "package":package, [m
[31m-                    "import": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": package.get("id"), [m
[31m-                    "activity_type": "new package"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-                multi_df.loc[multi_df['name'] == pkg_meta['name'], 'success'] = '1'[m
[31m-            except Exception as err:[m
[31m-                multi_df.loc[multi_df['name'] == pkg_meta['name'], 'success'] = '0'[m
[31m-                log_str = 'package_error: '+datetime.datetime.now().isoformat()+' -- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(pkg_meta['name'])+' : '+str(err)+'\n'[m
[31m-                activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "activity_type": "changed user"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-[m
[31m-        try:[m
[31m-            resource_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp3_Resource_Multi')[m
[31m-            resource_df.drop(0, inplace=True)[m
[31m-            resource_df.columns = ['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-        except:[m
[31m-            resource_df = pd.DataFrame(columns=['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'])[m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        try:[m
[31m-            final_df = pd.merge(multi_df,resource_df,how='left',left_on='dataset_name',right_on='dataset_name')[m
[31m-            final_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = final_df[(final_df['resource_url'] != '') & (final_df['success'] == '1')][m
[31m-            resource_df = resource_df[['name','success','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect']][m
[31m-            resource_df.columns = ['package_id','success','name','url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df["resource_created_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_last_updated_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df['created'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df['last_modified'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            resource_dict_list = resource_df.to_dict('records')[m
[31m-[m
[31m-            for resource_dict in resource_dict_list:[m
[31m-                res_meta = resource_dict[m
[31m-                resource = portal.action.resource_create(**res_meta)[m
[31m-                log.info('resource_create: '+datetime.datetime.now().isoformat()+' -- '+str(resource)+'\n')[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-[m
[31m-    def _other_type_process(self, data_dict):[m
[31m-        try:[m
[31m-            other_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp2_Meta_Other')[m
[31m-            other_df.drop(0, inplace=True)[m
[31m-            other_df["data_type"] = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏≠‡∏∑‡πà‡∏ô‡πÜ'[m
[31m-[m
[31m-            other_df.columns = ['name','data_type_other','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'][m
[31m-            other_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            other_df = other_df.astype('unicode')[m
[31m-            other_df = other_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-            other_df['high_value_dataset'] = np.where(other_df['high_value_dataset'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            other_df['reference_data'] = np.where(other_df['reference_data'].str.contains(u'‡πÑ‡∏°‡πà'), "False", "True")[m
[31m-            [m
[31m-            other_df["dataset_name"] = other_df["name"][m
[31m-            other_df["name"] = other_df["name"].str.lower()[m
[31m-            other_df["name"].replace('\s', '-', regex=True, inplace=True)[m
[31m-            if data_dict['template_org'] != 'all':[m
[31m-                other_df = other_df.loc[other_df['owner_org'] == data_dict['template_org']][m
[31m-                other_df.reset_index(drop=True, inplace=True)[m
[31m-            other_df["owner_org"] = data_dict['owner_org'][m
[31m-            other_df["private"] = True[m
[31m-            other_df["allow_harvest"] = "False"[m
[31m-            other_df['tag_string'] = other_df['tag_string'].str.split(',').apply(lambda x: [e.strip() for e in x]).tolist()[m
[31m-[m
[31m-            other_df["created_date"] = pd.to_datetime((pd.to_numeric(other_df["created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+other_df["created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            other_df["last_updated_date"] = pd.to_datetime((pd.to_numeric(other_df["last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+other_df["last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-[m
[31m-            objective_choices = [u'‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏°‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥', u'‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡∏Ñ‡∏á‡πÅ‡∏´‡πà‡∏á‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡πÅ‡∏°‡πà‡∏ö‡∏ó‡∏†‡∏≤‡∏¢‡πÉ‡∏ï‡πâ‡∏¢‡∏∏‡∏ó‡∏ò‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÅ‡∏ú‡∏ô‡∏õ‡∏è‡∏¥‡∏£‡∏π‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡πÅ‡∏ú‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà 3 (‡∏°‡∏ï‡∏¥‡∏Ñ‡∏£‡∏°. 4 ‡∏ò.‡∏Ñ. 2560)',u'‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•/‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡πà‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≤‡∏¢‡∏Å‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡∏°‡∏ï‡∏¥‡∏Ñ‡∏ì‡∏∞‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ',u'‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô',u'‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á',u'‡∏û‡∏±‡∏ô‡∏ò‡∏Å‡∏¥‡∏à‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô',u'‡∏î‡∏±‡∏ä‡∏ô‡∏µ/‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏≤‡∏ô‡∏≤‡∏ä‡∏≤‡∏ï‡∏¥',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            other_df['objective_other'] = other_df['objective'].isin(objective_choices)[m
[31m-            other_df['objective_other'] = np.where(other_df['objective_other'], 'True', other_df['objective'])[m
[31m-            other_df['objective'] = np.where(other_df['objective_other'] == 'True', other_df['objective'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['objective_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            update_frequency_unit_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', u'‡∏õ‡∏µ', u'‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏õ‡∏µ',u'‡πÑ‡∏ï‡∏£‡∏°‡∏≤‡∏™',u'‡πÄ‡∏î‡∏∑‡∏≠‡∏ô',u'‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå',u'‡∏ß‡∏±‡∏ô',u'‡∏ß‡∏±‡∏ô‡∏ó‡∏≥‡∏Å‡∏≤‡∏£',u'‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á',u'‡∏ô‡∏≤‡∏ó‡∏µ',u'‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏à‡∏£‡∏¥‡∏á',u'‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'][m
[31m-            other_df['update_frequency_unit_other'] = other_df['update_frequency_unit'].isin(update_frequency_unit_choices)[m
[31m-            other_df['update_frequency_unit_other'] = np.where(other_df['update_frequency_unit_other'], 'True', other_df['update_frequency_unit'])[m
[31m-            other_df['update_frequency_unit'] = np.where(other_df['update_frequency_unit_other'] == 'True', other_df['update_frequency_unit'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['update_frequency_unit_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            geo_coverage_choices = [u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡πÇ‡∏•‡∏Å', u'‡∏ó‡∏ß‡∏µ‡∏õ/‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÉ‡∏ô‡∏ó‡∏ß‡∏µ‡∏õ',u'‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à',u'‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏†‡∏≤‡∏Ñ',u'‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î',u'‡∏≠‡∏≥‡πÄ‡∏†‡∏≠',u'‡∏ï‡∏≥‡∏ö‡∏•',u'‡∏´‡∏°‡∏π‡πà‡∏ö‡πâ‡∏≤‡∏ô',u'‡πÄ‡∏ó‡∏®‡∏ö‡∏≤‡∏•/‡∏≠‡∏ö‡∏ï.',u'‡∏û‡∏¥‡∏Å‡∏±‡∏î',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            other_df['geo_coverage_other'] = other_df['geo_coverage'].isin(geo_coverage_choices)[m
[31m-            other_df['geo_coverage_other'] = np.where(other_df['geo_coverage_other'], 'True', other_df['geo_coverage'])[m
[31m-            other_df['geo_coverage'] = np.where(other_df['geo_coverage_other'] == 'True', other_df['geo_coverage'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['geo_coverage_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_format_choices = [u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö', 'Database', 'CSV','XML','Image','Video','Audio','Text','JSON','HTML','DOC/DOCX','XLS','PDF','RDF','NoSQL','Arc/Info Coverage','Shapefile','GeoTiff','GML'][m
[31m-            other_df['data_format_other'] = other_df['data_format'].isin(data_format_choices)[m
[31m-            other_df['data_format_other'] = np.where(other_df['data_format_other'], 'True', other_df['data_format'])[m
[31m-            other_df['data_format'] = np.where(other_df['data_format_other'] == 'True', other_df['data_format'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['data_format_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            license_id_choices = ['Open Data Common', 'Creative Commons Attributions','Creative Commons Attribution Non-Commercial','Creative Commons Attribution Share-Alike','Creative Commons Attribution Non-Commercial Share-Alike','Creative Commons Attribution No-Derivs','Creative Commons Attribution Non-Commercial No-Derivs'][m
[31m-            other_df['license_id_other'] = other_df['license_id'].isin(license_id_choices)[m
[31m-            other_df['license_id_other'] = np.where(other_df['license_id_other'], 'True', other_df['license_id'])[m
[31m-            other_df['license_id'] = np.where(other_df['license_id_other'] == 'True', other_df['license_id'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['license_id_other'].replace('True', '', regex=True, inplace=True)[m
[31m-            [m
[31m-            data_support_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê', u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏≠‡∏Å‡∏ä‡∏ô',u'‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô/‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®',u'‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ò‡∏¥/‡∏™‡∏°‡∏≤‡∏Ñ‡∏°',u'‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤'][m
[31m-            other_df['data_support_other'] = other_df['data_support'].isin(data_support_choices)[m
[31m-            other_df['data_support_other'] = np.where(other_df['data_support_other'], 'True', other_df['data_support'])[m
[31m-            other_df['data_support'] = np.where(other_df['data_support_other'] == 'True', other_df['data_support'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['data_support_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_collect_choices = ['',u'‡πÑ‡∏°‡πà‡∏°‡∏µ',u'‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•', u'‡∏Ñ‡∏£‡∏±‡∏ß‡πÄ‡∏£‡∏∑‡∏≠‡∏ô/‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß', u'‡∏ö‡πâ‡∏≤‡∏ô/‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏≤‡∏®‡∏±‡∏¢',u'‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó/‡∏´‡πâ‡∏≤‡∏á‡∏£‡πâ‡∏≤‡∏ô/‡∏™‡∏ñ‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£',u'‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£/‡∏™‡∏¥‡πà‡∏á‡∏õ‡∏•‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á',u'‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡∏©‡∏ï‡∏£ ‡∏õ‡∏£‡∏∞‡∏°‡∏á ‡∏õ‡πà‡∏≤‡πÑ‡∏°‡πâ',u'‡∏™‡∏±‡∏ï‡∏ß‡πå‡πÅ‡∏•‡∏∞‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏û‡∏∑‡∏ä',u'‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡πÄ‡∏ä‡∏¥‡∏á‡∏†‡∏π‡∏°‡∏¥‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà',u'‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ô‡πâ‡∏≥ ‡πÄ‡∏ä‡πà‡∏ô ‡πÅ‡∏°‡πà‡∏ô‡πâ‡∏≥ ‡∏≠‡πà‡∏≤‡∏á‡πÄ‡∏Å‡πá‡∏ö‡∏ô‡πâ‡∏≥',u'‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡∏¥‡∏ô‡∏ó‡∏≤‡∏á ‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡∏ô‡∏ô ‡∏ó‡∏≤‡∏á‡∏£‡∏ñ‡πÑ‡∏ü',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            other_df['data_collect_other'] = other_df['data_collect'].isin(data_collect_choices)[m
[31m-            other_df['data_collect_other'] = np.where(other_df['data_collect_other'], 'True', other_df['data_collect'])[m
[31m-            other_df['data_collect'] = np.where(other_df['data_collect_other'] == 'True', other_df['data_collect'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['data_collect_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            data_language_choices = ['',u'‡πÑ‡∏ó‡∏¢', u'‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©', u'‡∏à‡∏µ‡∏ô',u'‡∏°‡∏•‡∏≤‡∏¢‡∏π',u'‡∏û‡∏°‡πà‡∏≤',u'‡∏•‡∏≤‡∏ß',u'‡πÄ‡∏Ç‡∏°‡∏£',u'‡∏ç‡∏µ‡πà‡∏õ‡∏∏‡πà‡∏ô',u'‡πÄ‡∏Å‡∏≤‡∏´‡∏•‡∏µ',u'‡∏ù‡∏£‡∏±‡πà‡∏á‡πÄ‡∏®‡∏™',u'‡πÄ‡∏¢‡∏≠‡∏£‡∏°‡∏±‡∏ô',u'‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å',u'‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö'][m
[31m-            other_df['data_language_other'] = other_df['data_language'].isin(data_language_choices)[m
[31m-            other_df['data_language_other'] = np.where(other_df['data_language_other'], 'True', other_df['data_language'])[m
[31m-            other_df['data_language'] = np.where(other_df['data_language_other'] == 'True', other_df['data_language'], u'‡∏≠‡∏∑‡πà‡∏ô‡πÜ')[m
[31m-            other_df['data_language_other'].replace('True', '', regex=True, inplace=True)[m
[31m-[m
[31m-            other_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            [m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-            other_df = pd.DataFrame(columns=['name','data_type_other','title','owner_org','maintainer','maintainer_email','tag_string','notes','objective','update_frequency_unit','update_frequency_interval','geo_coverage','data_source','data_format','data_category','license_id','accessible_condition','created_date','last_updated_date','url','data_support','data_collect','data_language','high_value_dataset','reference_data','data_type'])[m
[31m-            other_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            other_df = other_df.astype('unicode')[m
[31m-            other_df = other_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        portal = LocalCKAN()[m
[31m-[m
[31m-        package_dict_list = other_df.to_dict('records')[m
[31m-        for pkg_meta in package_dict_list:[m
[31m-            try:[m
[31m-                if pkg_meta['data_language'] == '':[m
[31m-                    pkg_meta.pop('data_language', None)[m
[31m-                    pkg_meta.pop('data_language_other', None)[m
[31m-                package = portal.action.package_create(**pkg_meta)[m
[31m-                log_str = 'package_create: '+datetime.datetime.now().isoformat()+' -- ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(package.get("name"))+' ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à\n'[m
[31m-                activity_dict = {"data": {"actor": six.ensure_text(data_dict["importer"]), "package":package, [m
[31m-                    "import": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": package.get("id"), [m
[31m-                    "activity_type": "new package"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-                other_df.loc[other_df['name'] == pkg_meta['name'], 'success'] = '1'[m
[31m-            except Exception as err:[m
[31m-                other_df.loc[other_df['name'] == pkg_meta['name'], 'success'] = '0'[m
[31m-                log_str = 'package_error: '+datetime.datetime.now().isoformat()+' -- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '+str(pkg_meta['name'])+' : '+str(err)+'\n'[m
[31m-                activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Running", "import_log": log_str}, [m
[31m-                    "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-                    "activity_type": "changed user"[m
[31m-                    }[m
[31m-                portal.action.activity_create(**activity_dict)[m
[31m-                log.info(log_str)[m
[31m-[m
[31m-        try:[m
[31m-            resource_df = pd.read_excel(data_dict['filename'], header=[3], sheet_name='Temp3_Resource_Other')[m
[31m-            resource_df.drop(0, inplace=True)[m
[31m-            resource_df.columns = ['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-        except:[m
[31m-            resource_df = pd.DataFrame(columns=['dataset_name','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'])[m
[31m-            resource_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = resource_df.astype('unicode')[m
[31m-            resource_df = resource_df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)[m
[31m-[m
[31m-        try:[m
[31m-            final_df = pd.merge(other_df,resource_df,how='left',left_on='dataset_name',right_on='dataset_name')[m
[31m-            final_df.replace(np.nan, '', regex=True, inplace=True)[m
[31m-            resource_df = final_df[(final_df['resource_url'] != '') & (final_df['success'] == '1')][m
[31m-            resource_df = resource_df[['name','success','resource_name','resource_url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect']][m
[31m-            resource_df.columns = ['package_id','success','name','url','description','resource_accessible_condition','resource_last_updated_date','format','resource_created_date','resource_data_collect'][m
[31m-            resource_df["resource_created_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_created_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_created_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df["resource_last_updated_date"] = pd.to_datetime((pd.to_numeric(resource_df["resource_last_updated_date"].str.slice(stop=4), errors='coerce').astype('Int64')-543).astype(str)+resource_df["resource_last_updated_date"].str.slice(start=4), errors='coerce').astype(str)[m
[31m-            resource_df['created'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df['last_modified'] = datetime.datetime.utcnow().isoformat()[m
[31m-            resource_df.replace('NaT', '', regex=True, inplace=True)[m
[31m-            resource_dict_list = resource_df.to_dict('records')[m
[31m-[m
[31m-            for resource_dict in resource_dict_list:[m
[31m-                res_meta = resource_dict[m
[31m-                resource = portal.action.resource_create(**res_meta)[m
[31m-                log.info('resource_create: '+datetime.datetime.now().isoformat()+' -- '+str(resource)+'\n')[m
[31m-        except Exception as err:[m
[31m-            log.info(err)[m
[31m-    [m
[31m-    def _finished_process(self, data_dict):[m
[31m-        portal = LocalCKAN()[m
[31m-        log_str = 'import finished: '+datetime.datetime.now().isoformat()+' -- ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô\n'[m
[31m-        activity_dict = {"data": {"import_id": data_dict["import_uuid"], "import_status": "Finished", "import_log": log_str}, [m
[31m-            "user_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-            "object_id": model.User.by_name(six.ensure_text(data_dict["importer"])).id, [m
[31m-            "activity_type": "changed user"[m
[31m-            }[m
[31m-        portal.action.activity_create(**activity_dict)[m
[31m-        log.info(log_str)[m
[31m-[m
[31m-    def import_dataset(self):[m
[31m-[m
[31m-        context = {'model': model, 'user': c.user, 'auth_user_obj': c.userobj}[m
[31m-        try:[m
[31m-            check_access('config_option_update', context, {})[m
[31m-        except logic.NotAuthorized:[m
[31m-            abort(403, _('Need to be system administrator to administer'))[m
[31m-[m
[31m-        items = [[m
[31m-            {'name': 'template_file', 'control': 'image_upload', 'label': _('Template File'), 'placeholder': '', 'upload_enabled':h.uploads_enabled(),[m
[31m-                'field_url': 'template_file', 'field_upload': 'template_file_upload', 'field_clear': 'clear_template_file_upload'},[m
[31m-        ][m
[31m-        data = request.POST[m
[31m-        if 'save' in data:[m
[31m-            try:[m
[31m-                # really?[m
[31m-                data_dict = logic.clean_dict([m
[31m-                    dict_fns.unflatten([m
[31m-                        logic.tuplize_dict([m
[31m-                            logic.parse_params([m
[31m-                                request.POST, ignore_keys=CACHE_PARAMETERS))))[m
[31m-[m
[31m-                del data_dict['save'][m
[31m-[m
[31m-                schema = schema_.update_configuration_schema()[m
[31m-[m
[31m-                upload = uploader.get_uploader('admin')[m
[31m-                upload.update_data_dict(data_dict, 'template_file',[m
[31m-                                    'template_file_upload', 'clear_template_file_upload')[m
[31m-                upload.upload(uploader.get_max_image_size())[m
[31m-[m
[31m-                data, errors = _validate(data_dict, schema, context)[m
[31m-                if errors:[m
[31m-                    model.Session.rollback()[m
[31m-                    raise ValidationError(errors)[m
[31m-[m
[31m-                for key, value in six.iteritems(data):[m
[31m-                [m
[31m-                    if key == 'template_file' and value and not value.startswith('http')\[m
[31m-                            and not value.startswith('/'):[m
[31m-                        image_path = 'uploads/admin/'[m
[31m-[m
[31m-                        value = h.url_for_static('{0}{1}'.format(image_path, value))[m
[31m-[m
[31m-                    # Update CKAN's `config` object[m
[31m-                    config[key] = value[m
[31m-[m
[31m-                log.info('Import Dataset: {0}'.format(data))[m
[31m-                [m
[31m-                import_uuid = str(uuid.uuid4())[m
[31m-                filename = str(config['ckan.storage_path'])+'/storage/uploads/admin/'+data['template_file'][m
[31m-                template_org = data['template_org'] or 'all'[m
[31m-                owner_org = data['import_org'][m
[31m-                importer = c.user[m
[31m-                data_dict = {"import_uuid":import_uuid, "template_org":template_org, "owner_org":owner_org, "filename":filename, "importer":importer}[m
[31m-                log.info('Prepare to import data import_id:%r file:%r org:%r to_org:%r user:%r',import_uuid, filename, template_org, owner_org, importer)[m
[31m-[m
[31m-                row_count = 0[m
[31m-[m
[31m-                record_df = pd.read_excel(filename, header=[3], sheet_name='Temp2_Meta_Record')[m
[31m-                if template_org != 'all':[m
[31m-                    row_count += record_df.iloc[:, 3].tolist().count(template_org)[m
[31m-                else:[m
[31m-                    row_count += (len(record_df.index)-1)[m
[31m-                [m
[31m-                stat_df = pd.read_excel(filename, header=[3], sheet_name='Temp2_Meta_Stat')[m
[31m-                if template_org != 'all':[m
[31m-                    row_count += stat_df.iloc[:, 3].tolist().count(template_org)[m
[31m-                else:[m
[31m-                    row_count += (len(stat_df.index)-1)[m
[31m-[m
[31m-                gis_df = pd.read_excel(filename, header=[3], sheet_name='Temp2_Meta_GIS')[m
[31m-                if template_org != 'all':[m
[31m-                    row_count += gis_df.iloc[:, 3].tolist().count(template_org)[m
[31m-                else:[m
[31m-                    row_count += (len(gis_df.index)-1)[m
[31m-[m
[31m-                multi_df = pd.read_excel(filename, header=[3], sheet_name='Temp2_Meta_Multi')[m
[31m-                if template_org != 'all':[m
[31m-                    row_count += multi_df.iloc[:, 3].tolist().count(template_org)[m
[31m-                else:[m
[31m-                    row_count += (len(multi_df.index)-1)[m
[31m-[m
[31m-                other_df = pd.read_excel(filename, header=[3], sheet_name='Temp2_Meta_Other')[m
[31m-                if template_org != 'all':[m
[31m-                    row_count += other_df.iloc[:, 3].tolist().count(template_org)[m
[31m-                else:[m
[31m-                    row_count += (len(other_df.index)-1)[m
[31m- [m
[31m-                toolkit.get_action('dataset_bulk_import')(context, data_dict)[m
[31m-[m
[31m-                data_dict['row'] = row_count[m
[31m-                config["import_log"] = ''[m
[31m-                config['ckan.import_params'] = data_dict[m
[31m-                config['ckan.import_uuid'] = import_uuid[m
[31m-                config['ckan.import_row'] = row_count[m
[31m-[m
[31m-                model.set_system_info('ckan.import_params', data_dict)[m
[31m-                model.set_system_info('ckan.import_uuid', import_uuid)[m
[31m-                model.set_system_info('ckan.import_row', row_count)[m
[31m-            except logic.ValidationError as e:[m
[31m-                errors = e.error_dict[m
[31m-                error_summary = e.error_summary[m
[31m-                vars = {'data': data, 'errors': errors,[m
[31m-                        'error_summary': error_summary, 'form_items': items}[m
[31m-                return render('admin/dataset_import_form.html', extra_vars=vars)[m
[31m-[m
[31m-            h.redirect_to(controller='ckanext.thai_gdc.controllers.dataset:DatasetImportController', action='import_dataset')[m
[31m-[m
[31m-        schema = logic.schema.update_configuration_schema()[m
[31m-        data = {}[m
[31m-        for key in schema:[m
[31m-            data[key] = config.get(key)[m
[31m-[m
[31m-        vars = {'data': data, 'errors': {}, 'form_items': items}[m
[31m-        return render('admin/dataset_import_form.html', extra_vars=vars)[m
[31m-    [m
[31m-    def clear_import_log(self):[m
[31m-        [m
[31m-        config["import_log"] = ''[m
[31m-        config['template_file'] = ''[m
[31m-        config['import_org'] = ''[m
[31m-        config['template_org'] = ''[m
[31m-        config['ckan.import_params'] = ''[m
[31m-        config['ckan.import_uuid'] = ''[m
[31m-        config['ckan.import_row'] = ''[m
[31m-[m
[31m-        return render('admin/clear_import_log.html')[m
[31m-[m
[1mdiff --git a/ckanext/thai_gdc/controllers/export_package.py b/ckanext/thai_gdc/controllers/export_package.py[m
[1mdeleted file mode 100644[m
[1mindex dab719e..0000000[m
[1m--- a/ckanext/thai_gdc/controllers/export_package.py[m
[1m+++ /dev/null[m
[36m@@ -1,108 +0,0 @@[m
[31m-# -*- coding: utf-8 -*-[m
[31m-import os[m
[31m-import ckan.plugins as p[m
[31m-import ckan.lib.helpers as h[m
[31m-from ckan.plugins.toolkit import _[m
[31m-[m
[31m-import paste.fileapp[m
[31m-import pandas as pd[m
[31m-[m
[31m-[m
[31m-class ExportPackageController(p.toolkit.BaseController):[m
[31m-[m
[31m-    def __init__(self):[m
[31m-        my_path = p.toolkit.config.get('ckan.storage_path', None)[m
[31m-        if not my_path:[m
[31m-            p.toolkit.abort(404, _('Page Not Found'))[m
[31m-[m
[31m-        self.export_path = '%s/storage/uploads/admin_export' % my_path[m
[31m-[m
[31m-    def index(self):[m
[31m-[m
[31m-        is_access = h.check_access('config_option_update')[m
[31m-        if not is_access:[m
[31m-            p.toolkit.abort(404, _('Page Not Found'))[m
[31m-[m
[31m-        if os.path.isdir(self.export_path):[m
[31m-            for filename in os.listdir(self.export_path):[m
[31m-                file_path = os.path.join(self.export_path, filename)[m
[31m-                if os.path.isfile(file_path):[m
[31m-                    os.unlink(file_path)[m
[31m-[m
[31m-        return p.toolkit.render('admin/export_package.html')[m
[31m-[m
[31m-    def download(self, id=None):[m
[31m-        is_access = h.check_access('config_option_update')[m
[31m-        if not is_access:[m
[31m-            p.toolkit.abort(404, _('Page Not Found'))[m
[31m-[m
[31m-        if id is None:[m
[31m-            p.toolkit.abort(404, _('Page Not Found'))[m
[31m-[m
[31m-        file_path = '%s/%s.xlsx' % (self.export_path, id)[m
[31m-        rec_sheet_name = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô'[m
[31m-        sta_sheet_name = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥'[m
[31m-        gis_sheet_name = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏π‡∏°‡∏¥‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®‡πÄ‡∏ä‡∏¥‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà'[m
[31m-        oth_sheet_name = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏≠‡∏∑‡πà‡∏ô‡πÜ'[m
[31m-        mlt_sheet_name = u'‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó'[m
[31m-[m
[31m-        with pd.ExcelWriter(file_path) as writer:[m
[31m-            rec_csv = '%s/%s_rec.csv' % (self.export_path, id)[m
[31m-            if os.path.isfile(rec_csv):[m
[31m-                try:[m
[31m-                    df_record = pd.read_csv(rec_csv, keep_default_na=False, error_bad_lines=False)[m
[31m-                    df_record.to_excel(writer, encoding='utf-8', sheet_name=rec_sheet_name)[m
[31m-                except:[m
[31m-                    pass[m
[31m-                os.unlink(rec_csv)[m
[31m-[m
[31m-            sta_csv = '%s/%s_sta.csv' % (self.export_path, id)[m
[31m-            if os.path.isfile(sta_csv):[m
[31m-                try:[m
[31m-                    df_stat = pd.read_csv(sta_csv, keep_default_na=False, error_bad_lines=False)[m
[31m-                    df_stat.to_excel(writer, encoding='utf-8', sheet_name=sta_sheet_name)[m
[31m-                except:[m
[31m-                    pass[m
[31m-                os.unlink(sta_csv)[m
[31m-[m
[31m-            gis_csv = '%s/%s_gis.csv' % (self.export_path, id)[m
[31m-            if os.path.isfile(gis_csv):[m
[31m-                try:[m
[31m-                    df_gis = pd.read_csv(gis_csv, keep_default_na=False, error_bad_lines=False)[m
[31m-                    df_gis.to_excel(writer, encoding='utf-8', sheet_name=gis_sheet_name)[m
[31m-                except:[m
[31m-                    pass[m
[31m-                os.unlink(gis_csv)[m
[31m-[m
[31m-            oth_csv = '%s/%s_oth.csv' % (self.export_path, id)[m
[31m-            if os.path.isfile(oth_csv):[m
[31m-                try:[m
[31m-                    df_other = pd.read_csv(oth_csv, keep_default_na=False, error_bad_lines=False)[m
[31m-                    df_other.to_excel(writer, encoding='utf-8', sheet_name=oth_sheet_name)[m
[31m-                except:[m
[31m-                    pass[m
[31m-                os.unlink(oth_csv)[m
[31m-[m
[31m-            mlt_csv = '%s/%s_mlt.csv' % (self.export_path, id)[m
[31m-            if os.path.isfile(mlt_csv):[m
[31m-                try:[m
[31m-                    df_multi = pd.read_csv(mlt_csv, keep_default_na=False, error_bad_lines=False)[m
[31m-                    df_multi.to_excel(writer, encoding='utf-8', sheet_name=mlt_sheet_name)[m
[31m-                except:[m
[31m-                    pass[m
[31m-                os.unlink(mlt_csv)[m
[31m-[m
[31m-        if not os.path.exists(file_path):[m
[31m-            p.toolkit.abort(404, _('Page Not Found'))[m
[31m-[m
[31m-        fileapp = paste.fileapp.FileApp(file_path)[m
[31m-[m
[31m-        try:[m
[31m-            status, headers, app_iter = p.toolkit.request.call_application(fileapp)[m
[31m-        except OSError:[m
[31m-            p.toolkit.abort(404, _('Resource data not found'))[m
[31m-[m
[31m-        os.unlink(file_path)[m
[31m-        p.toolkit.response.content_type = 'application/application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'[m
[31m-        p.toolkit.response.headers['Content-disposition'] = 'attachment; filename=dataset.xlsx'[m
[31m-        return app_iter[m
[1mdiff --git a/ckanext/thai_gdc/controllers/organization.py b/ckanext/thai_gdc/controllers/organization.py[m
[1mdeleted file mode 100644[m
[1mindex 081a718..0000000[m
[1m--- a/ckanext/thai_gdc/controllers/organization.py[m
[1m+++ /dev/null[m
[36m@@ -1,142 +0,0 @@[m
[31m-# encoding: utf-8[m
[31m-[m
[31m-import logging[m
[31m-import re[m
[31m-[m
[31m-import ckan.lib.base as base[m
[31m-import ckan.lib.helpers as h[m
[31m-import ckan.logic as logic[m
[31m-import ckan.model as model[m
[31m-import ckan.lib.plugins as lib_plugins[m
[31m-import ckan.plugins as plugins[m
[31m-from ckan.plugins.toolkit import ([m
[31m-    _, c, BaseController, check_access, NotAuthorized, abort, render,[m
[31m-    redirect_to, request,[m
[31m-    )[m
[31m-from ckan.common import g, config, _[m
[31m-NotAuthorized = logic.NotAuthorized[m
[31m-ValidationError = logic.ValidationError[m
[31m-check_access = logic.check_access[m
[31m-get_action = logic.get_action[m
[31m-[m
[31m-log = logging.getLogger(__name__)[m
[31m-[m
[31m-lookup_group_plugin = lib_plugins.lookup_group_plugin[m
[31m-[m
[31m-is_org = True[m
[31m-[m
[31m-def _get_group_template(template_type, group_type=None):[m
[31m-    group_plugin = lookup_group_plugin(group_type)[m
[31m-    method = getattr(group_plugin, template_type)[m
[31m-    try:[m
[31m-        return method(group_type)[m
[31m-    except TypeError as err:[m
[31m-        if u'takes 1' not in str(err) and u'takes exactly 1' not in str(err):[m
[31m-            raise[m
[31m-        return method()[m
[31m-[m
[31m-def _replace_group_org(string):[m
[31m-    u''' substitute organization for group if this is an org'''[m
[31m-    if is_org:[m
[31m-        return re.sub(u'^group', u'organization', string)[m
[31m-    return string[m
[31m-[m
[31m-def _action(action_name):[m
[31m-    u''' select the correct group/org action '''[m
[31m-    return get_action(_replace_group_org(action_name))[m
[31m-[m
[31m-def _check_access(action_name, *args, **kw):[m
[31m-    u''' select the correct group/org check_access '''[m
[31m-    return check_access(_replace_group_org(action_name), *args, **kw)[m
[31m-[m
[31m-class OrganizationCustomController(plugins.toolkit.BaseController):[m
[31m-    def index(self):[m
[31m-        group_type = 'organization'[m
[31m-        is_organization = True[m
[31m-        extra_vars = {}[m
[31m-        page = h.get_page_number(request.params) or 1[m
[31m-        items_per_page = int(config.get(u'ckan.datasets_per_page', 20))[m
[31m-[m
[31m-        context = {[m
[31m-            u'model': model,[m
[31m-            u'session': model.Session,[m
[31m-            u'user': g.user,[m
[31m-            u'for_view': True,[m
[31m-            u'with_private': False[m
[31m-        }[m
[31m-[m
[31m-        try:[m
[31m-            _check_access(u'site_read', context)[m
[31m-            _check_access(u'group_list', context)[m
[31m-        except NotAuthorized:[m
[31m-            base.abort(403, _(u'Not authorized to see this page'))[m
[31m-[m
[31m-        q = request.params.get(u'q', u'')[m
[31m-        if q=='':[m
[31m-            extra_vars["page"] = h.Page([], 0)[m
[31m-            extra_vars["group_type"] = group_type[m
[31m-            return base.render([m
[31m-                _get_group_template(u'index_template', group_type), extra_vars)[m
[31m-        sort_by = request.params.get(u'sort')[m
[31m-[m
[31m-        # TODO: Remove[m
[31m-        # ckan 2.9: Adding variables that were removed from c object for[m
[31m-        # compatibility with templates in existing extensions[m
[31m-        g.q = q[m
[31m-        g.sort_by_selected = sort_by[m
[31m-[m
[31m-        extra_vars["q"] = q[m
[31m-        extra_vars["sort_by_selected"] = sort_by[m
[31m-[m
[31m-        # pass user info to context as needed to view private datasets of[m
[31m-        # orgs correctly[m
[31m-        if g.userobj:[m
[31m-            context['user_id'] = g.userobj.id[m
[31m-            context['user_is_admin'] = g.userobj.sysadmin[m
[31m-[m
[31m-        try:[m
[31m-            data_dict_global_results = {[m
[31m-                u'all_fields': False,[m
[31m-                u'q': q,[m
[31m-                u'sort': sort_by,[m
[31m-                u'type': group_type or u'group',[m
[31m-            }[m
[31m-            global_results = _action(u'group_list')(context,[m
[31m-                                                    data_dict_global_results)[m
[31m-        except ValidationError as e:[m
[31m-            if e.error_dict and e.error_dict.get(u'message'):[m
[31m-                msg = e.error_dict['message'][m
[31m-            else:[m
[31m-                msg = str(e)[m
[31m-            h.flash_error(msg)[m
[31m-            extra_vars["page"] = h.Page([], 0)[m
[31m-            extra_vars["group_type"] = group_type[m
[31m-            return base.render([m
[31m-                _get_group_template(u'index_template', group_type), extra_vars)[m
[31m-[m
[31m-        data_dict_page_results = {[m
[31m-            u'all_fields': True,[m
[31m-            u'q': q,[m
[31m-            u'sort': sort_by,[m
[31m-            u'type': group_type or u'group',[m
[31m-            u'limit': items_per_page,[m
[31m-            u'offset': items_per_page * (page - 1),[m
[31m-            u'include_extras': True[m
[31m-        }[m
[31m-        page_results = _action(u'group_list')(context, data_dict_page_results)[m
[31m-[m
[31m-        extra_vars["page"] = h.Page([m
[31m-            collection=global_results,[m
[31m-            page=page,[m
[31m-            url=h.pager_url,[m
[31m-            items_per_page=items_per_page, )[m
[31m-[m
[31m-        extra_vars["page"].items = page_results[m
[31m-        extra_vars["group_type"] = group_type[m
[31m-[m
[31m-        # TODO: Remove[m
[31m-        # ckan 2.9: Adding variables that were removed from c object for[m
[31m-        # compatibility with templates in existing extensions[m
[31m-        g.page = extra_vars["page"][m
[31m-        return base.render([m
[31m-            _get_group_template(u'index_template', group_type), extra_vars)[m
[1mdiff --git a/ckanext/thai_gdc/controllers/popup.py b/ckanext/thai_gdc/controllers/popup.py[m
[1mdeleted file mode 100644[m
[1mindex 113729e..0000000[m
[1m--- a/ckanext/thai_gdc/controllers/popup.py[m
[1m+++ /dev/null[m
[36m@@ -1,46 +0,0 @@[m
[31m-import ckan.plugins as p[m
[31m-import ckan.lib.helpers as h[m
[31m-from ckan.plugins.toolkit import _[m
[31m-import ckan.lib.uploader as uploader[m
[31m-[m
[31m-[m
[31m-class PopupController(p.toolkit.BaseController):[m
[31m-[m
[31m-    def index(self, data=None, errors=None, error_summary=None):[m
[31m-        is_access = h.check_access('config_option_update')[m
[31m-        if not is_access:[m
[31m-            p.toolkit.abort(404, _('Page Not Found'))[m
[31m-[m
[31m-        if p.toolkit.request.method == 'POST' and not data:[m
[31m-            data = dict(p.toolkit.request.POST)[m
[31m-            upload = uploader.get_uploader('admin')[m
[31m-            upload.update_data_dict(data, 'EVENT_IMAGE', 'EVENT_IMAGE_UPLOAD', 'EVENT_IMAGE_CLEAR')[m
[31m-            upload.upload(uploader.get_max_image_size())[m
[31m-[m
[31m-            data_dict_event = {[m
[31m-                'fields': {[m
[31m-                    'EVENT_IMAGE': data['EVENT_IMAGE'],[m
[31m-                    'EVENT_TEXT': data['EVENT_TEXT'],[m
[31m-                    'EVENT_URL': data['EVENT_URL'],[m
[31m-                    'EVENT_PUBLIC': data['EVENT_PUBLIC'][m
[31m-                },[m
[31m-                'conf_group': 'EVENT'[m
[31m-            }[m
[31m-            try:[m
[31m-                p.toolkit.get_action('gdc_agency_update_conf_group')(data_dict=data_dict_event)[m
[31m-            except p.toolkit.ValidationError as e:[m
[31m-                errors = e.error_dict[m
[31m-                error_summary = e.error_summary[m
[31m-                return self.index(data, errors, error_summary)[m
[31m-[m
[31m-        if data is None:[m
[31m-            data = p.toolkit.get_action('gdc_agency_get_conf_group')(data_dict={'conf_group': 'EVENT'})[m
[31m-[m
[31m-        data.update({[m
[31m-            'EVENT_IMAGE_IS_URL': data and 'EVENT_IMAGE' in data.keys() and data['EVENT_IMAGE'].startswith('http')[m
[31m-        })[m
[31m-[m
[31m-        errors = errors or {}[m
[31m-        error_summary = error_summary or {}[m
[31m-        extra_vars = {'data': data, 'errors': errors, 'error_summary': error_summary}[m
[31m-        return p.toolkit.render('admin/popup.html', extra_vars=extra_vars)[m
[1mdiff --git a/ckanext/thai_gdc/controllers/user.py b/ckanext/thai_gdc/controllers/user.py[m
[1mdeleted file mode 100644[m
[1mindex ad69c6b..0000000[m
[1m--- a/ckanext/thai_gdc/controllers/user.py[m
[1m+++ /dev/null[m
[36m@@ -1,49 +0,0 @@[m
[31m-# -*- coding: utf-8 -*-[m
[31m-import ckan.plugins as plugins[m
[31m-import ckan.lib.helpers as helpers[m
[31m-import ckan.logic as logic[m
[31m-import ckan.lib.navl.dictization_functions as dict_fns[m
[31m-import ckan.model as model[m
[31m-import logging[m
[31m-from ckan.common import g[m
[31m-[m
[31m-from ckan.plugins.toolkit import ([m
[31m-    _, c, h, BaseController, check_access, NotAuthorized, abort, render,[m
[31m-    redirect_to, request,[m
[31m-    )[m
[31m-[m
[31m-from ckan.controllers.home import CACHE_PARAMETERS[m
[31m-[m
[31m-_validate = dict_fns.validate[m
[31m-ValidationError = logic.ValidationError[m
[31m-[m
[31m-log = logging.getLogger(__name__)[m
[31m-[m
[31m-class UserManageController(plugins.toolkit.BaseController):[m
[31m-[m
[31m-    def user_active(self):[m
[31m-        data = request.GET[m
[31m-        if 'id' in data:[m
[31m-            try:[m
[31m-                data_dict = logic.clean_dict([m
[31m-                    dict_fns.unflatten([m
[31m-                        logic.tuplize_dict([m
[31m-                            logic.parse_params([m
[31m-                                request.GET, ignore_keys=CACHE_PARAMETERS))))[m
[31m-                [m
[31m-                context = {[m
[31m-                    u'model': model,[m
[31m-                    u'session': model.Session,[m
[31m-                    u'user': g.user,[m
[31m-                    u'auth_user_obj': g.userobj,[m
[31m-                    u'for_view': True[m
[31m-                }[m
[31m-                check_access('user_update', context, {})[m
[31m-                user_dict = plugins.toolkit.get_action('user_show')(None, {'id':data['id']})[m
[31m-                if user_dict and user_dict['state'] == 'deleted':[m
[31m-                    user = model.User.get(user_dict['name'])[m
[31m-                    user.state = model.State.ACTIVE[m
[31m-                    user.save()[m
[31m-                h.redirect_to(controller='user', action='read', id=data['id'])[m
[31m-            except logic.ValidationError as e:[m
[31m-                return e[m
[1mdiff --git a/ckanext/thai_gdc/helpers.py b/ckanext/thai_gdc/helpers.py[m
[1mindex 986cae2..ead6c68 100644[m
[1m--- a/ckanext/thai_gdc/helpers.py[m
[1m+++ b/ckanext/thai_gdc/helpers.py[m
[36m@@ -4,8 +4,7 @@[m
 import ckan.plugins.toolkit as toolkit[m
 import ckan.logic as logic[m
 import ckan.model as model[m
[31m-from pylons import config[m
[31m-from ckan.common import _, c[m
[32m+[m[32mfrom ckan.common import _, c, g, config[m
 import ckan.lib.helpers as h[m
 import json[m
 import os[m
[36m@@ -39,7 +38,7 @@[m [mdef dataset_bulk_import_log(import_id):[m
 def dataset_bulk_import_status(import_id):[m
     try:[m
         context = {'model': model,[m
[31m-                    'user': c.user, 'auth_user_obj': c.userobj}[m
[32m+[m[32m                    'user': g.user, 'auth_user_obj': g.userobj}[m
 [m
         like_q1 = u'%' + import_id + u'%'[m
         like_q2 = u'%Finished%'[m
[36m@@ -234,7 +233,7 @@[m [mdef get_action(action_name, data_dict=None):[m
     return logic.get_action(action_name)({}, data_dict)[m
 [m
 def get_organizations(all_fields=False, include_dataset_count=False, sort="name asc"):[m
[31m-    context = {'user': c.user}[m
[32m+[m[32m    context = {'user': g.user}[m
     data_dict = {[m
         'all_fields': all_fields,[m
         'include_dataset_count': include_dataset_count,[m
[36m@@ -242,7 +241,7 @@[m [mdef get_organizations(all_fields=False, include_dataset_count=False, sort="name[m
     return logic.get_action('organization_list')(context, data_dict)[m
 [m
 def get_groups(all_fields=False, include_dataset_count=False, sort="name asc"):[m
[31m-    context = {'user': c.user}[m
[32m+[m[32m    context = {'user': g.user}[m
     data_dict = {[m
         'all_fields': all_fields,[m
         'include_dataset_count': include_dataset_count,[m
[36m@@ -338,7 +337,7 @@[m [mdef get_all_groups():[m
     groups = toolkit.get_action('group_list')([m
         data_dict={'include_dataset_count': False, 'all_fields': True})[m
     pkg_group_ids = set(group['id'] for group[m
[31m-                        in c.pkg_dict.get('groups', []))[m
[32m+[m[32m                        in g.pkg_dict.get('groups', []))[m
     return [[group['id'], group['display_name']][m
             for group in groups if[m
             group['id'] not in pkg_group_ids][m
[36m@@ -347,7 +346,7 @@[m [mdef get_all_groups_all_type(type=None):[m
     user_groups = opend_model.get_groups_all_type(type)[m
 [m
     pkg_group_ids = set(group['id'] for group[m
[31m-                            in c.pkg_dict.get('groups', []))[m
[32m+[m[32m                            in g.pkg_dict.get('groups', []))[m
     return [[group['id'], group['display_name']][m
                             for group in user_groups if[m
                             group['id'] not in pkg_group_ids][m
[36m@@ -360,7 +359,7 @@[m [mdef users_in_organization(organization_id):[m
         .filter(model.Member.group_id == organization_id)[m
     users = query.all()[m
     context = {'model': model,[m
[31m-                    'user': c.user, 'auth_user_obj': c.userobj}[m
[32m+[m[32m                    'user': g.user, 'auth_user_obj': g.userobj}[m
     users_list = [][m
     for user in users:[m
         users_list.append(model_dictize.member_dictize(user, context))[m
[1mdiff --git a/ckanext/thai_gdc/i18n/th/LC_MESSAGES/ckanext-thai_gdc.mo b/ckanext/thai_gdc/i18n/th/LC_MESSAGES/ckanext-thai_gdc.mo[m
[1mindex 8b7796f..80d7fae 100644[m
Binary files a/ckanext/thai_gdc/i18n/th/LC_MESSAGES/ckanext-thai_gdc.mo and b/ckanext/thai_gdc/i18n/th/LC_MESSAGES/ckanext-thai_gdc.mo differ
[1mdiff --git a/ckanext/thai_gdc/i18n/th/LC_MESSAGES/ckanext-thai_gdc.po b/ckanext/thai_gdc/i18n/th/LC_MESSAGES/ckanext-thai_gdc.po[m
[1mindex 1c585a4..dca91da 100644[m
[1m--- a/ckanext/thai_gdc/i18n/th/LC_MESSAGES/ckanext-thai_gdc.po[m
[1m+++ b/ckanext/thai_gdc/i18n/th/LC_MESSAGES/ckanext-thai_gdc.po[m
[36m@@ -266,5 +266,8 @@[m [mmsgstr "‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å"[m
 msgid "Show"[m
 msgstr "‡πÅ‡∏™‡∏î‡∏á"[m
 [m
[31m-msgid "Data Class Level"[m
[31m-msgstr "‡∏ä‡∏±‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê"[m
\ No newline at end of file[m
[32m+[m[32mmsgid "Data Classification"[m
[32m+[m[32mmsgstr "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"[m
[32m+[m
[32m+[m[32mmsgid "Column"[m
[32m+[m[32mmsgstr "‡∏ü‡∏¥‡∏•‡∏î‡πå"[m
\ No newline at end of file[m
[1mdiff --git a/ckanext/thai_gdc/plugin.py b/ckanext/thai_gdc/plugin.py[m
[1mindex a78e65e..9cbc931 100644[m
[1m--- a/ckanext/thai_gdc/plugin.py[m
[1m+++ b/ckanext/thai_gdc/plugin.py[m
[36m@@ -19,6 +19,8 @@[m [mfrom ckanext.thai_gdc import validation as thai_gdc_validator[m
 import logging[m
 import os[m
 [m
[32m+[m[32mfrom ckanext.thai_gdc import blueprint[m
[32m+[m
 log = logging.getLogger(__name__)[m
 [m
 class Thai_GDCPlugin(plugins.SingletonPlugin, DefaultTranslation, toolkit.DefaultDatasetForm):[m
[36m@@ -28,16 +30,20 @@[m [mclass Thai_GDCPlugin(plugins.SingletonPlugin, DefaultTranslation, toolkit.Defaul[m
     plugins.implements(plugins.ITemplateHelpers)[m
     plugins.implements(plugins.IPackageController, inherit=True)[m
     plugins.implements(plugins.IValidators)[m
[31m-    plugins.implements(plugins.IRoutes, inherit=True)[m
     plugins.implements(plugins.IResourceController, inherit=True)[m
     plugins.implements(plugins.IFacets, inherit=True)[m
     plugins.implements(plugins.IActions)[m
[32m+[m[32m    plugins.implements(plugins.IBlueprint)[m
[32m+[m
[32m+[m[32m    # IBlueprint[m
[32m+[m[32m    def get_blueprint(self):[m
[32m+[m[32m        return blueprint.thai_gdc_blueprint[m
 [m
     # IFacets[m
     def dataset_facets(self, facets_dict, package_type):[m
         facets_dict['data_type'] = toolkit._('Dataset Type') #‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•[m
         facets_dict['data_category'] = toolkit._('Data Category') #‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ï‡∏≤‡∏°‡∏ò‡∏£‡∏£‡∏°‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•[m
[31m-        facets_dict['data_class_level'] = toolkit._('Data Class Level') #‡∏ä‡∏±‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê[m
[32m+[m[32m        facets_dict['data_classification'] = toolkit._('Data Classification') #‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ä‡∏±‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•[m
         facets_dict['private'] = toolkit._('Visibility') #‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á[m
         return facets_dict[m
 [m
[36m@@ -60,7 +66,7 @@[m [mclass Thai_GDCPlugin(plugins.SingletonPlugin, DefaultTranslation, toolkit.Defaul[m
         return search_results[m
 [m
     def before_view(self, pkg_dict):[m
[31m-        pkg_dict['tracking_summary'] = (model.TrackingSummary.get_for_package(pkg_dict['id']))[m
[32m+[m[32m        pkg_dict['tracking_summary'] = model.TrackingSummary.get_for_package(pkg_dict['id'])[m
         return pkg_dict[m
 [m
     def _isEnglish(self, s):[m
[36m@@ -129,16 +135,10 @@[m [mclass Thai_GDCPlugin(plugins.SingletonPlugin, DefaultTranslation, toolkit.Defaul[m
 [m
     # IConfigurer[m
     def update_config(self, config_):[m
[31m-        if toolkit.check_ckan_version(max_version='2.9'):[m
[31m-            toolkit.add_ckan_admin_tab(config_, 'banner_edit', '‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏ö‡∏ô‡πÄ‡∏ô‡∏≠‡∏£‡πå')[m
[31m-            toolkit.add_ckan_admin_tab(config_, 'dataset_import', '‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•')[m
[31m-            toolkit.add_ckan_admin_tab(config_, 'gdc_agency_admin_export', '‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•')[m
[31m-            toolkit.add_ckan_admin_tab(config_, 'gdc_agency_admin_popup', '‡∏õ‡πá‡∏≠‡∏õ‡∏≠‡∏±‡∏û')[m
[31m-        else:[m
[31m-            toolkit.add_ckan_admin_tab(config_, 'banner_edit', u'‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏ö‡∏ô‡πÄ‡∏ô‡∏≠‡∏£‡πå', icon='wrench')[m
[31m-            toolkit.add_ckan_admin_tab(config_, 'dataset_import', u'‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', icon='cloud-upload')[m
[31m-            toolkit.add_ckan_admin_tab(config_, 'gdc_agency_admin_export', u'‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', icon='cloud-download')[m
[31m-            toolkit.add_ckan_admin_tab(config_, 'gdc_agency_admin_popup', u'‡∏õ‡πá‡∏≠‡∏õ‡∏≠‡∏±‡∏û', icon='window-maximize')[m
[32m+[m[32m        toolkit.add_ckan_admin_tab(config_, 'thai_gdc.edit_banner', u'‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏ö‡∏ô‡πÄ‡∏ô‡∏≠‡∏£‡πå', icon='wrench')[m
[32m+[m[32m        toolkit.add_ckan_admin_tab(config_, 'thai_gdc.import_dataset', u'‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', icon='cloud-upload')[m
[32m+[m[32m        toolkit.add_ckan_admin_tab(config_, 'thai_gdc.export_dataset_init', u'‡∏™‡πà‡∏á‡∏≠‡∏≠‡∏Å‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•', icon='cloud-download')[m
[32m+[m[32m        toolkit.add_ckan_admin_tab(config_, 'thai_gdc.edit_popup', u'‡∏õ‡πá‡∏≠‡∏õ‡∏≠‡∏±‡∏û', icon='window-maximize')[m
 [m
         toolkit.add_template_directory(config_, 'templates')[m
         toolkit.add_public_directory(config_, 'public')[m
[36m@@ -212,75 +212,6 @@[m [mclass Thai_GDCPlugin(plugins.SingletonPlugin, DefaultTranslation, toolkit.Defaul[m
         })[m
         return schema[m
 [m
[31m-    # IRoutes[m
[31m-    def before_map(self, map):[m
[31m-        map.connect([m
[31m-            'banner_edit',[m
[31m-            '/ckan-admin/banner-edit',[m
[31m-            action='edit_banner',[m
[31m-            ckan_icon='wrench',[m
[31m-            controller='ckanext.thai_gdc.controllers.banner:BannerEditController',[m
[31m-        )[m
[31m-        map.connect([m
[31m-            'dataset_import',[m
[31m-            '/ckan-admin/dataset-import',[m
[31m-            action='import_dataset',[m
[31m-            ckan_icon='cloud-upload',[m
[31m-            controller='ckanext.thai_gdc.controllers.dataset:DatasetImportController',[m
[31m-        )[m
[31m-        map.connect([m
[31m-            'clear_import_log',[m
[31m-            '/ckan-admin/clear-import-log',[m
[31m-            action='clear_import_log',[m
[31m-            controller='ckanext.thai_gdc.controllers.dataset:DatasetImportController',[m
[31m-        )[m
[31m-        map.connect([m
[31m-            'dataset_datatype_patch',[m
[31m-            '/dataset/edit-datatype/{package_id}',[m
[31m-            action='datatype_patch',[m
[31m-            controller='ckanext.thai_gdc.controllers.dataset:DatasetManageController',[m
[31m-        )[m
[31m-        map.connect([m
[31m-            'user_active',[m
[31m-            '/user/edit/user_active',[m
[31m-            action='user_active',[m
[31m-            controller='ckanext.thai_gdc.controllers.user:UserManageController',[m
[31m-        )[m
[31m-        # map.connect([m
[31m-        #     'organizations_index',[m
[31m-        #     '/organization/',[m
[31m-        #     action='index',[m
[31m-        #     controller='ckanext.thai_gdc.controllers.organization:OrganizationCustomController'[m
[31m-        # )[m
[31m-        # map.connect([m
[31m-        #     'organizations_index',[m
[31m-        #     '/organization',[m
[31m-        #     action='index',[m
[31m-        #     controller='ckanext.thai_gdc.controllers.organization:OrganizationCustomController'[m
[31m-        # )[m
[31m-        map.connect([m
[31m-            'gdc_agency_admin_export',[m
[31m-            '/ckan-admin/dataset-export',[m
[31m-            action='index',[m
[31m-            ckan_icon='file',[m
[31m-            controller='ckanext.thai_gdc.controllers.export_package:ExportPackageController'[m
[31m-        )[m
[31m-        map.connect([m
[31m-            'gdc_agency_admin_download',[m
[31m-            '/ckan-admin/dataset-export/{id:.*|}',[m
[31m-            action='download',[m
[31m-            ckan_icon='file',[m
[31m-            controller='ckanext.thai_gdc.controllers.export_package:ExportPackageController'[m
[31m-        )[m
[31m-        map.connect([m
[31m-            'gdc_agency_admin_popup',[m
[31m-            '/ckan-admin/dataset-popup',[m
[31m-            action='index',[m
[31m-            ckan_icon='file',[m
[31m-            controller='ckanext.thai_gdc.controllers.popup:PopupController'[m
[31m-        )[m
[31m-        return map[m
[31m-[m
     # IAuthFunctions[m
     def get_auth_functions(self):[m
         auth_functions = {[m
[1mdiff --git a/ckanext/thai_gdc/public/base/admin/thai-gdc-update.json b/ckanext/thai_gdc/public/base/admin/thai-gdc-update.json[m
[1mindex 63d6ca3..0287abf 100644[m
[1m--- a/ckanext/thai_gdc/public/base/admin/thai-gdc-update.json[m
[1m+++ b/ckanext/thai_gdc/public/base/admin/thai-gdc-update.json[m
[36m@@ -1,4 +1,4 @@[m
 {[m
[31m-  "date": "2024-04-04",[m
[31m-  "version": "2.2.0"[m
[32m+[m[32m  "date": "2024-06-20",[m
[32m+[m[32m  "version": "2.2.1"[m
 }[m
[1mdiff --git a/ckanext/thai_gdc/templates/admin/popup.html b/ckanext/thai_gdc/templates/admin/popup.html[m
[1mindex 0be0f5b..9617f81 100644[m
[1m--- a/ckanext/thai_gdc/templates/admin/popup.html[m
[1m+++ b/ckanext/thai_gdc/templates/admin/popup.html[m
[36m@@ -42,7 +42,7 @@[m
 [m
 [m
     <div class="form-actions">[m
[31m-        <button type="submit" class="btn btn-primary" >{{ _('Update Config') }}</button>[m
[32m+[m[32m        <button type="submit" class="btn btn-primary" name="save">{{ _('Update Config') }}</button>[m
     </div>[m
 [m
 [m
[1mdiff --git a/ckanext/thai_gdc/templates/group/snippets/group_item.html b/ckanext/thai_gdc/templates/group/snippets/group_item.html[m
[1mindex 5f34ef1..8769d1c 100644[m
[1m--- a/ckanext/thai_gdc/templates/group/snippets/group_item.html[m
[1m+++ b/ckanext/thai_gdc/templates/group/snippets/group_item.html[m
[36m@@ -16,7 +16,7 @@[m
   {% block link %}[m
     {{ super() }}[m
   {% endblock %}[m
[31m-  {% if group.user_member or (h.check_access('member_delete', group) and c.controller in ['package', 'dataset'] and c.action in ['groups']) %}[m
[32m+[m[32m  {% if group.user_member or (h.check_access('member_delete', group) and request.endpoint == 'dataset.groups') %}[m
     <input name="group_remove.{{ group.id }}" value="{{ _('Remove') }}" type="submit" class="btn btn-danger btn-sm media-edit" title="{{ _('Remove dataset from this group') }}"/>[m
   {% endif %}[m
 {% endblock %}[m
[1mdiff --git a/ckanext/thai_gdc/templates/package/search.html b/ckanext/thai_gdc/templates/package/search.html[m
[1mindex fb867c9..cd997b1 100644[m
[1m--- a/ckanext/thai_gdc/templates/package/search.html[m
[1m+++ b/ckanext/thai_gdc/templates/package/search.html[m
[36m@@ -8,7 +8,7 @@[m
       {{ h.snippet('snippets/facet_list.html', title=facet_titles['tags'], name='tags', search_facets=search_facets) }}[m
       {{ h.snippet('snippets/facet_list.html', title=facet_titles['data_type'], name='data_type', search_facets=search_facets) }}[m
       {{ h.snippet('snippets/facet_list.html', title=facet_titles['data_category'], name='data_category', search_facets=search_facets) }}[m
[31m-      {{ h.snippet('snippets/facet_list.html', title=facet_titles['data_class_level'], name='data_class_level', search_facets=search_facets) }}[m
[32m+[m[32m      {{ h.snippet('snippets/facet_list.html', title=facet_titles['data_classification'], name='data_classification', search_facets=search_facets) }}[m[41m[m
       {{ h.snippet('snippets/facet_list.html', title=facet_titles['private'], name='private', search_facets=search_facets) }}[m
       {{ h.snippet('snippets/facet_list.html', title=facet_titles['res_format'], name='res_format', search_facets=search_facets) }}[m
       {{ h.snippet('snippets/facet_list.html', title=facet_titles['license_id'], name='license_id', search_facets=search_facets) }}[m
[1mdiff --git a/ckanext/thai_gdc/templates/package/snippets/dictionary_table.html b/ckanext/thai_gdc/templates/package/snippets/dictionary_table.html[m
[1mindex 5ea4f80..618d5c8 100644[m
[1m--- a/ckanext/thai_gdc/templates/package/snippets/dictionary_table.html[m
[1m+++ b/ckanext/thai_gdc/templates/package/snippets/dictionary_table.html[m
[36m@@ -2,11 +2,10 @@[m
   <td>{{ field.id }}</td>[m
   <td>[m
     {% set override = h.get_translated(field.get('info', {}), 'type_override') %}[m
[31m-    {{ field.type }}[m
     {% if override!='' %}[m
[31m-      <br/><u>override:</u> {{ override }}[m
[32m+[m[32m      {{ override }} (override)[m
     {% else %}[m
[31m-      [m
[32m+[m[32m      {{ field.type }}[m
     {% endif %}[m
   </td>[m
   <td>{{ h.get_translated(field.get('info', {}), 'label') }}</td>[m
[1mdiff --git a/ckanext/thai_gdc/templates/scheming/form_snippets/data_category.html b/ckanext/thai_gdc/templates/scheming/form_snippets/data_category.html[m
[1mindex 0921ca2..e20437f 100644[m
[1m--- a/ckanext/thai_gdc/templates/scheming/form_snippets/data_category.html[m
[1m+++ b/ckanext/thai_gdc/templates/scheming/form_snippets/data_category.html[m
[36m@@ -3,8 +3,9 @@[m
 <script>[m
   function sSelect_data_category(){[m
       index = document.getElementById('field-data_category').value;[m
[31m-      var sDataClassLevel = document.getElementById("field-data_class_level");[m
[32m+[m[32m      var sDataClassLevel = document.getElementById("field-data_classification");[m
       if(index == '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞'){[m
[32m+[m[32m        document.getElementById('field-accessible_condition').value = '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•';[m
         for (var i = 0; i < sDataClassLevel.length; i++) {[m
           var txt = sDataClassLevel.options[i].attributes[0].nodeValue;[m
           if(txt != '' && txt != '‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ú‡∏¢') {[m
[36m@@ -15,9 +16,10 @@[m
           }[m
         }[m
       } else if(index == '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô') {[m
[32m+[m[32m        document.getElementById('field-accessible_condition').value = '';[m
         for (var i = 0; i < sDataClassLevel.length; i++) {[m
           var txt = sDataClassLevel.options[i].attributes[0].nodeValue;[m
[31m-          if(txt != '' && txt != '‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£') {[m
[32m+[m[32m          if(txt != '' && txt != '‡∏ä‡∏±‡πâ‡∏ô‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£') {[m
             $(sDataClassLevel.options[i]).attr('disabled', 'disabled').hide();[m
           }[m
           else {[m
[36m@@ -25,9 +27,10 @@[m
           }[m
         }[m
       } else if(index == '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•') {[m
[32m+[m[32m        document.getElementById('field-accessible_condition').value = '';[m
         for (var i = 0; i < sDataClassLevel.length; i++) {[m
           var txt = sDataClassLevel.options[i].attributes[0].nodeValue;[m
[31m-          if(txt != '' && txt != '‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£' && txt != '‡∏•‡∏±‡∏ö' && txt != '‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏Å') {[m
[32m+[m[32m          if(txt != '' && txt != '‡∏ä‡∏±‡πâ‡∏ô‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£' && txt != '‡∏•‡∏±‡∏ö' && txt != '‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏Å') {[m
             $(sDataClassLevel.options[i]).attr('disabled', 'disabled').hide();[m
           }[m
           else {[m
[36m@@ -35,6 +38,7 @@[m
           }[m
         }[m
       } else {[m
[32m+[m[32m        document.getElementById('field-accessible_condition').value = '';[m
         for (var i = 0; i < sDataClassLevel.length; i++) {[m
           var txt = sDataClassLevel.options[i].attributes[0].nodeValue;[m
           if(txt != '' && txt != '‡∏•‡∏±‡∏ö' && txt != '‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏Å' && txt != '‡∏•‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î') {[m
[1mdiff --git a/ckanext/thai_gdc/templates/scheming/form_snippets/data_class_level.html b/ckanext/thai_gdc/templates/scheming/form_snippets/data_class_level.html[m
[1mdeleted file mode 100644[m
[1mindex 2575391..0000000[m
[1m--- a/ckanext/thai_gdc/templates/scheming/form_snippets/data_class_level.html[m
[1m+++ /dev/null[m
[36m@@ -1,83 +0,0 @@[m
[31m-{% import 'macros/form.html' as form %}[m
[31m-[m
[31m-<script>[m
[31m-  function sClick_data_class_level(){[m
[31m-      index = document.getElementById('field-data_category').value;[m
[31m-      var sDataClassLevel = document.getElementById("field-data_class_level");[m
[31m-      if(index == '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞'){[m
[31m-        for (var i = 0; i < sDataClassLevel.length; i++) {[m
[31m-          var txt = sDataClassLevel.options[i].attributes[0].nodeValue;[m
[31m-          if(txt != '' && txt != '‡πÄ‡∏õ‡∏¥‡∏î‡πÄ‡∏ú‡∏¢') {[m
[31m-            $(sDataClassLevel.options[i]).attr('disabled', 'disabled').hide();[m
[31m-          }[m
[31m-          else {[m
[31m-            $(sDataClassLevel.options[i]).removeAttr('disabled').show();[m
[31m-          }[m
[31m-        }[m
[31m-      } else if(index == '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô') {[m
[31m-        for (var i = 0; i < sDataClassLevel.length; i++) {[m
[31m-          var txt = sDataClassLevel.options[i].attributes[0].nodeValue;[m
[31m-          if(txt != '' && txt != '‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£') {[m
[31m-            $(sDataClassLevel.options[i]).attr('disabled', 'disabled').hide();[m
[31m-          }[m
[31m-          else {[m
[31m-            $(sDataClassLevel.options[i]).removeAttr('disabled').show();[m
[31m-          }[m
[31m-        }[m
[31m-      } else if(index == '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡πà‡∏ß‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•') {[m
[31m-        for (var i = 0; i < sDataClassLevel.length; i++) {[m
[31m-          var txt = sDataClassLevel.options[i].attributes[0].nodeValue;[m
[31m-          if(txt != '' && txt != '‡πÄ‡∏ú‡∏¢‡πÅ‡∏û‡∏£‡πà‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£' && txt != '‡∏•‡∏±‡∏ö' && txt != '‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏Å') {[m
[31m-            $(sDataClassLevel.options[i]).attr('disabled', 'disabled').hide();[m
[31m-          }[m
[31m-          else {[m
[31m-            $(sDataClassLevel.options[i]).removeAttr('disabled').show();[m
[31m-          }[m
[31m-        }[m
[31m-      } else {[m
[31m-        for (var i = 0; i < sDataClassLevel.length; i++) {[m
[31m-          var txt = sDataClassLevel.options[i].attributes[0].nodeValue;[m
[31m-          if(txt != '' && txt != '‡∏•‡∏±‡∏ö' && txt != '‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏Å' && txt != '‡∏•‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î') {[m
[31m-            $(sDataClassLevel.options[i]).attr('disabled', 'disabled').hide();[m
[31m-          }[m
[31m-          else {[m
[31m-            $(sDataClassLevel.options[i]).removeAttr('disabled').show();[m
[31m-          }[m
[31m-        }[m
[31m-      }[m
[31m-  }[m
[31m-  </script>[m
[31m-[m
[31m-{%- set options=[] -%}[m
[31m-{%- if not h.scheming_field_required(field) or[m
[31m-    field.get('form_include_blank_choice', false) -%}[m
[31m-  {%- do options.append({'value': '', 'text': ''}) -%}[m
[31m-{%- endif -%}[m
[31m-{%- for c in h.scheming_field_choices(field) -%}[m
[31m-    {%- do options.append({[m
[31m-      'value': c.value|string,[m
[31m-      'text': h.scheming_language_text(c.label) }) -%}[m
[31m-{%- endfor -%}[m
[31m-{%- if data[field.field_name] -%}[m
[31m-  {%- set option_selected = data[field.field_name]|string -%}[m
[31m-{%- else -%}[m
[31m-  {%- set option_selected = None -%}[m
[31m-{%- endif -%}[m
[31m-[m
[31m-{% call form.select([m
[31m-    field.field_name,[m
[31m-    id='field-' + field.field_name,[m
[31m-    label=h.scheming_language_text(field.label),[m
[31m-    options=options,[m
[31m-    selected=option_selected,[m
[31m-    error=errors[field.field_name],[m
[31m-    classes=['control-medium'],[m
[31m-    attrs=dict({"class": "form-control", 'onclick' : "sClick_data_class_level();"}, **(field.get('form_attrs', {}))),[m
[31m-    is_required=h.scheming_field_required(field)[m
[31m-    )[m
[31m-%}[m
[31m-[m
[31m-{% endcall %}[m
[31m-[m
[31m-[m
[31m-[m
[1mdiff --git a/ckanext/thai_gdc/templates/scheming/form_snippets/data_language.html b/ckanext/thai_gdc/templates/scheming/form_snippets/data_language.html[m
[1mindex 2ed5f80..63f5a9c 100644[m
[1m--- a/ckanext/thai_gdc/templates/scheming/form_snippets/data_language.html[m
[1m+++ b/ckanext/thai_gdc/templates/scheming/form_snippets/data_language.html[m
[36m@@ -1,5 +1,9 @@[m
 {% import 'macros/form.html' as form %}[m
 <script type="text/javascript">[m
[32m+[m[32m    index = document.getElementById('field-data_category').value;[m
[32m+[m[32m    if(index == '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞'){[m
[32m+[m[32m        document.getElementById('field-accessible_condition').value = '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•';[m
[32m+[m[32m    }[m
 	function sCheck_data_language(cb){[m
 		if(cb.value=="‡∏≠‡∏∑‡πà‡∏ô‡πÜ" && cb.checked==true){[m
             document.getElementById('field-data_language_other').value = '';[m
[1mdiff --git a/ckanext/thai_gdc/templates/user/edit_user_form.html b/ckanext/thai_gdc/templates/user/edit_user_form.html[m
[1mindex 1d5cf46..bbb037e 100644[m
[1m--- a/ckanext/thai_gdc/templates/user/edit_user_form.html[m
[1m+++ b/ckanext/thai_gdc/templates/user/edit_user_form.html[m
[36m@@ -49,7 +49,7 @@[m
       {% if h.check_access('user_delete', {'id': data.id}) and data.state != 'deleted' %}[m
         <a class="btn btn-danger pull-left" href="{% url_for 'user_delete', id=data.id %}" data-module="confirm-action" data-module-content="{{ _('Are you sure you want to delete this User?') }}">{% block delete_button_text %}{{ _('Delete') }}{% endblock %}</a>[m
       {% elif h.check_access('user_delete', {'id': data.id}) and data.state != 'active' %}[m
[31m-        <a class="btn btn-success pull-left" href="{% url_for 'user_active', id=data.id %}" data-module="confirm-action" data-module-content="{{ _('Are you sure you want to active this User?') }}">{{ _('Active') }}</a>[m
[32m+[m[32m        <a class="btn btn-success pull-left" href="{% url_for 'user_active', id=data.id %}" onclick="return confirm('Are you sure you want to active this User?')">{{ _('Active') }}</a>[m[41m[m
       {% endif %}[m
     {% endblock %}[m
     {% block generate_button %}[m
